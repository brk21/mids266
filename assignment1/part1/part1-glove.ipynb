{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GloVe: Global Vectors for Word Representation (25 points)\n",
    "\n",
    "[GloVe](http://nlp.stanford.edu/projects/glove/) \\[[PDF](http://nlp.stanford.edu/pubs/glove.pdf)\\] is (yet) another way to train word vectors.  Its main advantage relative to Word2Vec is its training speed.\n",
    "\n",
    "## Approach\n",
    "The intuition of the GloVe approach to training word vectors is as follows:\n",
    "\n",
    "1. From the training data, estimate $P(k | word)$.\n",
    "2. Notice that some words, $k$, are far more common than others in the context of $word$.\n",
    "3. In particular, in the table below, notice in the bottom row that $k$'s that are related to ice (vs. steam) result in quite large numbers where as those related to steam (vs. ice) are incredibly low.  Unrelated numbers are about 1.0.\n",
    "\n",
    "<img src=\"glove_table.png\">\n",
    "\n",
    "At a high level then training $F(w_i, w_j, \\tilde{w}_k) = \\frac{P_{ij}}{P_{jk}}$ seems like a useful thing to do.  In this case, F is a (simple) neural network accepting word vectors $w_i$ and $w_j$ for words $i$ and $j$, and a context vector $\\tilde{w}_k$ for word $k$.\n",
    "\n",
    "With some reasonable assumptions about desireable properties of vector embeddings (see Section 3 of the paper), the authors make this more concrete and simplify to a simple objective function based directly on the cooccurrence matrix instead of probabilities:\n",
    "\n",
    "$$J = \\sum\\limits_{i,j}^V f(X_{ij})(w_i^T\\tilde{w}_j + b_i + \\tilde{b}_j - log(X_{ij}))^2$$\n",
    "\n",
    "where $f(.)$ is the weight of the $j$'th word appearing in the $i$th word's context window $X_{ij}$ times.  This weighting function is described in detail immediately before Equation (9) in the paper.\n",
    "\n",
    "Note that $f(0) = 0$ pairs $i,j$ where $X_{ij} = 0$ can be skipped in the sum above.\n",
    "\n",
    "## vs. Word2Vec\n",
    "Similar to Word2Vec, GloVe embeds words in a vector space based on the \"[company it keeps](https://en.wikipedia.org/wiki/John_Rupert_Firth)\" - based on cooccurrance between words in small context windows.  Unlike Word2Vec which repeatedly iterates over the training data one context window at a time, GloVe does a single pass over the training data to collect cooccurrance statistics.  GloVe then trains entirely based on this table of counts.\n",
    "\n",
    "## The Plan\n",
    "In this assignment, you are going to train GloVe models and visualize them.\n",
    "\n",
    "1. Parsing utilities.\n",
    "\n",
    "2. Phrases.  Implement Section 4 (Equation 6) of the [Word2Vec](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) paper.  This allows us to treat \"los angeles\" as a single item in our vocabulary.\n",
    "\n",
    "3. Implement TensorFlow for GloVe & Train embeddings.\n",
    "\n",
    "5. Visualize embeddings.\n",
    "\n",
    "\n",
    "As usual, we begin by importing some useful libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'word_utils' from 'word_utils.pyc'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glove\n",
    "import glove_test\n",
    "import nltk\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import unittest\n",
    "import word_stream\n",
    "import word_stream_test\n",
    "import word_utils\n",
    "reload(glove)\n",
    "reload(glove_test)\n",
    "reload(word_stream)\n",
    "reload(word_stream_test)\n",
    "reload(word_utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Lower casing sometimes causes more harm than good,\n",
    "# but we do it here anyways in absence of more careful normalization.\n",
    "\n",
    "# The Brown corpus is only 1m tokens.  Test your code with this, then if you want, run with Wikipedia.\n",
    "\n",
    "# words = [w.lower() for w in nltk.corpus.brown.words()]\n",
    "\n",
    "# Use first billion bytes of Wikipedia, consisting of 17m tokens.  While this produces better\n",
    "# embeddings, all of the code runs correspondingly longer.  We recommend getting everything to work\n",
    "# with the Brown corpus before trying this.\n",
    "\n",
    "# If you are going to try this using Google Compute Engine, we recommend using the\n",
    "# n1-highcpu-16 (16 vCPUs, 14.4 GB memory) version.\n",
    "\n",
    "# You should not spend time debugging on this instance though or you will find yourself\n",
    "# without GCE credit!\n",
    "\n",
    "words = open('text8').read().split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['anarchism',\n",
       "  'originated',\n",
       "  'as',\n",
       "  'a',\n",
       "  'term',\n",
       "  'of',\n",
       "  'abuse',\n",
       "  'first',\n",
       "  'used',\n",
       "  'against'],\n",
       " 17005207)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[:10], len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  Parsing Utilities (8 points)\n",
    "\n",
    "In **word_stream.py** (find this in the same directory as this notebook), implement:\n",
    "1.  context_windows\n",
    "2.  cooccurrence_table\n",
    "\n",
    "You may find it helpful to run the follow unit tests to check your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_context_windows (word_stream_test.TestWordStreams) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.202s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=1 errors=0 failures=0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(word_stream)\n",
    "reload(word_stream_test)\n",
    "unittest.TextTestRunner(verbosity=2).run(\n",
    "    unittest.TestLoader().loadTestsFromName(\n",
    "        'TestWordStreams.test_context_windows', word_stream_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_cooccurrence_table (word_stream_test.TestWordStreams) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.001s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=1 errors=0 failures=0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(word_stream)\n",
    "reload(word_stream_test)\n",
    "unittest.TextTestRunner(verbosity=2).run(\n",
    "    unittest.TestLoader().loadTestsFromName(\n",
    "        'TestWordStreams.test_cooccurrence_table', word_stream_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  Phrases (4 points)\n",
    "\n",
    "Implement the function **score_bigram** in **word_stream.py**.\n",
    "Specifically, read the introduction to Section 4 of the [Word2Vec](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) paper and implement Equation 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_score_bigram (word_stream_test.TestWordStreams) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.001s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=1 errors=0 failures=0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(word_stream)\n",
    "reload(word_stream_test)\n",
    "unittest.TextTestRunner(verbosity=2).run(\n",
    "    unittest.TestLoader().loadTestsFromName(\n",
    "        'TestWordStreams.test_score_bigram', word_stream_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use your scoring code to see the best scoring phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.001957730812013348, ('burkina', 'faso')),\n",
       " (0.0018769551616266945, ('buenos', 'aires')),\n",
       " (0.0015462384775498067, ('costa', 'rica')),\n",
       " (0.001476912716754159, ('fran', 'ois')),\n",
       " (0.0014698871762275545, ('sri', 'lanka')),\n",
       " (0.0013311013855555331, ('las', 'vegas')),\n",
       " (0.00129174343010821, ('coca', 'cola')),\n",
       " (0.0012493492972410203, ('nova', 'scotia')),\n",
       " (0.0012483640390617136, ('saudi', 'arabia')),\n",
       " (0.0011100906440921615, ('monty', 'python')),\n",
       " (0.0010555160864711254, ('puerto', 'rico')),\n",
       " (0.001049632628579997, ('guant', 'namo')),\n",
       " (0.0010233918128654971, ('gda', 'sk')),\n",
       " (0.0008804754567466431, ('mario', 'kart')),\n",
       " (0.0008553519070943142, ('hip', 'hop')),\n",
       " (0.0007207648654192401, ('import', 'insubstantial')),\n",
       " (0.0007022071041776631, ('los', 'angeles')),\n",
       " (0.0006783348664121336, ('hong', 'kong')),\n",
       " (0.000629765412383887, ('walt', 'disney')),\n",
       " (0.0006038647342995169, ('ann', 'arbor')),\n",
       " (0.0005259159703149653, ('bene', 'gesserit')),\n",
       " (0.0005191225884396119, ('ronald', 'reagan')),\n",
       " (0.0005115641661034767, ('amino', 'acids')),\n",
       " (0.0005074865493609262, ('vice', 'versa')),\n",
       " (0.00048339984366643356, ('duplicate', 'info')),\n",
       " (0.00048192771084337347, ('encyclop', 'dia')),\n",
       " (0.00047006432459178625, ('bj', 'rk')),\n",
       " (0.0004453692549191399, ('anglo', 'saxon')),\n",
       " (0.00044122059264750007, ('infant', 'mortality')),\n",
       " (0.00041948120045649426, ('gregorian', 'calendar')),\n",
       " (0.00041688859079820357, ('carbon', 'dioxide')),\n",
       " (0.0004055895234111655, ('http', 'www')),\n",
       " (0.000374960279631395, ('mac', 'os')),\n",
       " (0.00035818535886467595, ('equatorial', 'guinea')),\n",
       " (0.00035357376349250824, ('bipolar', 'disorder')),\n",
       " (0.0003502155172413793, ('el', 'salvador')),\n",
       " (0.0003448019417081894, ('prize', 'laureate')),\n",
       " (0.0003316664088948463, ('super', 'bowl')),\n",
       " (0.0003305300748292829, ('adolf', 'hitler')),\n",
       " (0.00032976617776752455, ('san', 'francisco')),\n",
       " (0.00032346149356066753, ('nobel', 'prize')),\n",
       " (0.0003231633549327986, ('cape', 'verde')),\n",
       " (0.00032004963714617825, ('quarter', 'finalist')),\n",
       " (0.00031386917420379715, ('global', 'warming')),\n",
       " (0.00031242734247849337, ('lindy', 'hop')),\n",
       " (0.00030902348578491963, ('julius', 'caesar')),\n",
       " (0.00028796866900881183, ('winston', 'churchill')),\n",
       " (0.00027368609376464273, ('quantum', 'mechanics')),\n",
       " (0.0002558617936935185, ('von', 'neumann')),\n",
       " (0.0002527805864509606, ('bin', 'laden')),\n",
       " (0.0002519196275620226, ('grand', 'prix')),\n",
       " (0.00024368976769511365, ('martial', 'arts')),\n",
       " (0.00023923444976076556, ('schr', 'dinger')),\n",
       " (0.00023570059532042578, ('per', 'capita')),\n",
       " (0.0002338946805666934, ('elvis', 'presley')),\n",
       " (0.000231000231000231, ('avant', 'garde')),\n",
       " (0.00023056881326231814, ('blade', 'runner')),\n",
       " (0.0002249799159560005, ('star', 'trek')),\n",
       " (0.0002247765006385696, ('gaza', 'strip')),\n",
       " (0.00022148394241417498, ('khmer', 'rouge')),\n",
       " (0.00021824530772588389, ('anti', 'semitism')),\n",
       " (0.0002161953301808681, ('ms', 'dos')),\n",
       " (0.00020465894723437541, ('import', 'duplicate')),\n",
       " (0.00019837020444419477, ('big', 'bang')),\n",
       " (0.00019807680249890274, ('san', 'diego')),\n",
       " (0.00019533256533572956, ('prime', 'minister')),\n",
       " (0.00019305019305019305, ('notre', 'dame')),\n",
       " (0.000180150217566032, ('harry', 'potter')),\n",
       " (0.0001724237025116386, ('warner', 'bros')),\n",
       " (0.00017155225591216524, ('median', 'income')),\n",
       " (0.00017005441741357234, ('guinea', 'bissau')),\n",
       " (0.00016335664813511838, ('martin', 'luther')),\n",
       " (0.00015560360785744562, ('hiv', 'aids')),\n",
       " (0.0001541867454629975, ('al', 'qaeda')),\n",
       " (0.0001486767766874814, ('middle', 'ages')),\n",
       " (0.00014614541468761417, ('providers', 'isps')),\n",
       " (0.00013783054718727233, ('des', 'moines')),\n",
       " (0.00013678630728434488, ('abraham', 'lincoln')),\n",
       " (0.00013411397929368103, ('cd', 'rom')),\n",
       " (0.00013073302777692332, ('atlantic', 'ocean')),\n",
       " (0.00013031432831425724, ('krak', 'w')),\n",
       " (0.00013014196319151473, ('fourier', 'transform')),\n",
       " (0.00012743039358445568, ('supreme', 'court')),\n",
       " (0.00012511260134120709, ('iso', 'iec')),\n",
       " (0.00012432114113721128, ('import', 'jargon')),\n",
       " (0.00012070068028957891, ('align', 'center')),\n",
       " (0.00012052782389213375, ('artificial', 'intelligence')),\n",
       " (0.00011678569824372277, ('tcp', 'ip')),\n",
       " (0.00011672243405182475, ('bertrand', 'russell')),\n",
       " (0.00011161387631975868, ('floppy', 'disk')),\n",
       " (0.00010907624337751379, ('comic', 'strip')),\n",
       " (0.00010486577181208053, ('don', 'quixote')),\n",
       " (0.00010192972666857478, ('external', 'links')),\n",
       " (0.00010136784206578305, ('broadcast', 'stations')),\n",
       " (9.936477518719614e-05, ('nautical', 'miles')),\n",
       " (9.843453510436433e-05, ('directory', 'category')),\n",
       " (9.718229009243024e-05, ('didn', 't')),\n",
       " (9.640178152836381e-05, ('jesus', 'christ')),\n",
       " (9.57339995797044e-05, ('louis', 'xiv')),\n",
       " (9.443465122135482e-05, ('alpha', 'centauri')),\n",
       " (9.437790801929084e-05, ('fatty', 'acids')),\n",
       " (9.373670561146376e-05, ('doesn', 't')),\n",
       " (9.162727811113718e-05, ('runways', 'total')),\n",
       " (9.146954978687595e-05, ('intellectual', 'property')),\n",
       " (9.095250916737185e-05, ('et', 'al')),\n",
       " (9.005715980907882e-05, ('fa', 'cup')),\n",
       " (8.874846908890822e-05, ('ibm', 'pc')),\n",
       " (8.822718500137855e-05, ('angular', 'momentum')),\n",
       " (8.497832200782957e-05, ('science', 'fiction')),\n",
       " (8.481512423295322e-05, ('armed', 'forces')),\n",
       " (8.364924426630416e-05, ('pacific', 'ocean')),\n",
       " (8.126667659926047e-05, ('ice', 'hockey')),\n",
       " (8.004340131093304e-05, ('intelligent', 'design')),\n",
       " (7.952760602023978e-05, ('committed', 'suicide')),\n",
       " (7.904403217557074e-05, ('marvel', 'comics')),\n",
       " (7.85751702462022e-05, ('paved', 'runways')),\n",
       " (7.816067184617273e-05, ('de', 'facto')),\n",
       " (7.757661896977446e-05, ('wasn', 't')),\n",
       " (7.674246043443348e-05, ('red', 'sox')),\n",
       " (7.660886044691972e-05, ('washington', 'dc')),\n",
       " (7.658712576745364e-05, ('motion', 'picture')),\n",
       " (7.656695156695156e-05, ('sexual', 'orientation')),\n",
       " (7.642961596665631e-05, ('periodic', 'table')),\n",
       " (7.510750986929007e-05, ('ottoman', 'empire')),\n",
       " (7.504042994593005e-05, ('closely', 'related')),\n",
       " (7.502019774554688e-05, ('semi', 'finalist')),\n",
       " (7.489878542510122e-05, ('summer', 'olympics')),\n",
       " (7.426673450748514e-05, ('indo', 'european')),\n",
       " (7.18203935489794e-05, ('soviet', 'union')),\n",
       " (7.158675506476293e-05, ('isn', 't')),\n",
       " (7.152071239831056e-05, ('kinetic', 'energy')),\n",
       " (7.141440190438405e-05, ('mbox', 'mbox')),\n",
       " (7.12195374614767e-05, ('vast', 'majority')),\n",
       " (7.081434583812606e-05, ('henry', 'viii')),\n",
       " (7.03358537014243e-05, ('jean', 'baptiste')),\n",
       " (6.950110840728732e-05, ('academy', 'award')),\n",
       " (6.897890778286461e-05, ('indiana', 'jones')),\n",
       " (6.895838361548805e-05, ('queen', 'elizabeth')),\n",
       " (6.890036146477675e-05, ('holy', 'spirit')),\n",
       " (6.732415146501651e-05, ('dominican', 'republic')),\n",
       " (6.724492533309488e-05, ('twentieth', 'century')),\n",
       " (6.609041449939324e-05, ('albert', 'einstein')),\n",
       " (6.600971285774906e-05, ('physicist', 'nobel')),\n",
       " (6.52709134533796e-05, ('differential', 'equations')),\n",
       " (6.510628601191445e-05, ('capita', 'income')),\n",
       " (6.466951602504107e-05, ('final', 'fantasy')),\n",
       " (6.431419628692707e-05, ('molecular', 'biology')),\n",
       " (6.411737754582723e-05, ('est', 'airports')),\n",
       " (6.39049581118215e-05, ('user', 'interface')),\n",
       " (6.377718502511695e-05, ('czech', 'republic')),\n",
       " (6.345461117044823e-05, ('don', 't')),\n",
       " (6.334860738644762e-05, ('further', 'reading')),\n",
       " (6.140047664962614e-05, ('chicago', 'illinois')),\n",
       " (6.119569757381178e-05, ('frac', 'frac')),\n",
       " (5.98986856516977e-05, ('varying', 'degrees')),\n",
       " (5.940123554569935e-05, ('dc', 'comics')),\n",
       " (5.8749193697290584e-05, ('persian', 'gulf')),\n",
       " (5.8262521960489044e-05, ('kyoto', 'protocol')),\n",
       " (5.741359254322245e-05, ('benjamin', 'franklin')),\n",
       " (5.712849340737186e-05, ('census', 'bureau')),\n",
       " (5.706785367802317e-05, ('cue', 'ball')),\n",
       " (5.706534144639445e-05, ('life', 'expectancy')),\n",
       " (5.68955431118412e-05, ('w', 'bush')),\n",
       " (5.6624339972537195e-05, ('chemist', 'nobel')),\n",
       " (5.6474293708279616e-05, ('presidential', 'election')),\n",
       " (5.572804713000557e-05, ('mickey', 'mouse')),\n",
       " (5.516195204357794e-05, ('arable', 'land')),\n",
       " (5.502042539079576e-05, ('baltic', 'sea')),\n",
       " (5.4736534812436144e-05, ('saturday', 'night')),\n",
       " (5.446267128510119e-05, ('bwv', 'anh')),\n",
       " (5.439606311817738e-05, ('roman', 'catholic')),\n",
       " (5.400953268251847e-05, ('nineteenth', 'century')),\n",
       " (5.381882216431317e-05, ('microsoft', 'windows')),\n",
       " (5.3444909263469603e-05, ('vice', 'president')),\n",
       " (5.3072055521535004e-05, ('service', 'providers')),\n",
       " (5.274591316850561e-05, ('amino', 'acid')),\n",
       " (5.270092226613966e-05, ('professional', 'wrestler')),\n",
       " (5.252038553351393e-05, ('bob', 'dylan')),\n",
       " (5.150400472900407e-05, ('heavy', 'metal')),\n",
       " (5.100060978989966e-05, ('marine', 'corps')),\n",
       " (5.077665078242437e-05, ('indian', 'ocean')),\n",
       " (5.017740139331313e-05, ('butt', 'head')),\n",
       " (4.9951659684176605e-05, ('anti', 'semitic')),\n",
       " (4.9819227374953736e-05, ('falkland', 'islands')),\n",
       " (4.9779129108377735e-05, ('foreign', 'relations')),\n",
       " (4.9589470030249574e-05, ('th', 'century')),\n",
       " (4.958401868936089e-05, ('executive', 'branch')),\n",
       " (4.773675281241079e-05, ('car', 'driver')),\n",
       " (4.742005275480869e-05, ('shortly', 'thereafter')),\n",
       " (4.702410917527578e-05, ('old', 'testament')),\n",
       " (4.6879434541105237e-05, ('web', 'site')),\n",
       " (4.685310961272193e-05, ('north', 'carolina')),\n",
       " (4.6824915963465216e-05, ('couldn', 't')),\n",
       " (4.6036347230683535e-05, ('administrative', 'divisions')),\n",
       " (4.594261792976853e-05, ('human', 'beings')),\n",
       " (4.573507018988125e-05, ('project', 'gutenberg')),\n",
       " (4.555426788824992e-05, ('southeast', 'asia')),\n",
       " (4.539192686568025e-05, ('bill', 'clinton')),\n",
       " (4.5390102887193534e-05, ('united', 'states')),\n",
       " (4.415618946328864e-05, ('purchasing', 'power')),\n",
       " (4.301246434364462e-05, ('main', 'article')),\n",
       " (4.289681146468354e-05, ('wide', 'variety')),\n",
       " (4.206209746729672e-05, ('no', 'import')),\n",
       " (4.1411618547228655e-05, ('ethnic', 'groups')),\n",
       " (4.126510542618539e-05, ('years', 'ago')),\n",
       " (4.097179166189485e-05, ('white', 'sox')),\n",
       " (4.086294076494016e-05, ('royal', 'navy')),\n",
       " (4.081569044193003e-05, ('northern', 'ireland')),\n",
       " (4.028473416228518e-05, ('baseball', 'player')),\n",
       " (4.007813752441798e-05, ('endangered', 'species')),\n",
       " (3.993965573027891e-05, ('magnetic', 'field')),\n",
       " (3.9831380489262126e-05, ('military', 'manpower')),\n",
       " (3.97886427298192e-05, ('poverty', 'line')),\n",
       " (3.968368001817239e-05, ('floating', 'point')),\n",
       " (3.9423173269774415e-05, ('privy', 'council')),\n",
       " (3.9117653354117355e-05, ('climate', 'change')),\n",
       " (3.9068859647468735e-05, ('new', 'york')),\n",
       " (3.8730309787149285e-05, ('wouldn', 't')),\n",
       " (3.806800315812154e-05, ('internet', 'explorer')),\n",
       " (3.795057728739953e-05, ('new', 'zealand')),\n",
       " (3.775437006833541e-05, ('mobile', 'cellular')),\n",
       " (3.7400654511453954e-05, ('ad', 'hoc')),\n",
       " (3.733443639350064e-05, ('communist', 'party')),\n",
       " (3.697781475559253e-05, ('league', 'baseball')),\n",
       " (3.680655979442005e-05, ('united', 'kingdom')),\n",
       " (3.669954220719202e-05, ('object', 'oriented')),\n",
       " (3.6626280088489096e-05, ('air', 'force')),\n",
       " (3.629686565889487e-05, ('faroe', 'islands')),\n",
       " (3.615614559912958e-05, ('power', 'parity')),\n",
       " (3.607516860931705e-05, ('catholic', 'church')),\n",
       " (3.5952645997143274e-05, ('byzantine', 'empire')),\n",
       " (3.578267985766445e-05, ('native', 'americans')),\n",
       " (3.554401944531279e-05, ('growth', 'rate')),\n",
       " (3.543621986655732e-05, ('isaac', 'newton')),\n",
       " (3.5378457044153025e-05, ('eastern', 'orthodox')),\n",
       " (3.522029845139057e-05, ('death', 'penalty')),\n",
       " (3.518222446686431e-05, ('rather', 'than')),\n",
       " (3.515350363252871e-05, ('net', 'migration')),\n",
       " (3.4797649712000575e-05, ('hebrew', 'bible')),\n",
       " (3.448470588235294e-05, ('wide', 'range')),\n",
       " (3.385761334605559e-05, ('christopher', 'columbus')),\n",
       " (3.3700490832372026e-05, ('human', 'rights')),\n",
       " (3.3632828775595236e-05, ('operating', 'systems')),\n",
       " (3.342717703775786e-05, ('raw', 'materials')),\n",
       " (3.34043892413052e-05, ('health', 'care')),\n",
       " (3.332331109507217e-05, ('metropolitan', 'area')),\n",
       " (3.3199538623221394e-05, ('no', 'longer')),\n",
       " (3.317485775384733e-05, ('star', 'wars')),\n",
       " (3.302044448739787e-05, ('rural', 'areas')),\n",
       " (3.294961157115045e-05, ('st', 'petersburg')),\n",
       " (3.273825606877552e-05, ('takes', 'place')),\n",
       " (3.252862378683656e-05, ('south', 'korea')),\n",
       " (3.247125441837794e-05, ('south', 'africa')),\n",
       " (3.207630710121878e-05, ('game', 'boy')),\n",
       " (3.206255300664005e-05, ('square', 'kilometers')),\n",
       " (3.205308616494987e-05, ('exchange', 'rates')),\n",
       " (3.1985670419651997e-05, ('west', 'indies')),\n",
       " (3.1964028019330495e-05, ('natural', 'selection')),\n",
       " (3.167635569433498e-05, ('hockey', 'player')),\n",
       " (3.150983303727219e-05, ('en', 'route')),\n",
       " (3.135377611378738e-05, ('holy', 'roman')),\n",
       " (3.1321071666447205e-05, ('nazi', 'germany')),\n",
       " (3.125620938088147e-05, ('trade', 'unions')),\n",
       " (3.119784110939523e-05, ('british', 'isles')),\n",
       " (3.1018696786890854e-05, ('foreign', 'affairs')),\n",
       " (3.09825422706396e-05, ('foreign', 'policy')),\n",
       " (3.094308890644906e-05, ('official', 'website')),\n",
       " (3.093573564397592e-05, ('international', 'airport')),\n",
       " (3.0742217335927004e-05, ('roman', 'empire')),\n",
       " (3.0386326153231047e-05, ('nuclear', 'weapons')),\n",
       " (3.0062367976781395e-05, ('world', 'factbook')),\n",
       " (3.003837613100101e-05, ('nobel', 'peace')),\n",
       " (2.984516626518044e-05, ('north', 'america')),\n",
       " (2.9706189868173477e-05, ('na', 'na')),\n",
       " (2.9674479131218368e-05, ('political', 'parties')),\n",
       " (2.944423997055576e-05, ('prime', 'ministers')),\n",
       " (2.9377790459334246e-05, ('operating', 'system')),\n",
       " (2.9332275486610964e-05, ('days', 'remaining')),\n",
       " (2.9187570605728327e-05, ('olympic', 'games')),\n",
       " (2.900954172176465e-05, ('bits', 'added')),\n",
       " (2.897522530432767e-05, ('signal', 'processing')),\n",
       " (2.873232961728537e-05, ('eldest', 'son')),\n",
       " (2.858112478479014e-05, ('natural', 'gas')),\n",
       " (2.8293650365931213e-05, ('catholic', 'encyclopedia')),\n",
       " (2.7959956479719915e-05, ('general', 'relativity')),\n",
       " (2.7934598572770333e-05, ('civil', 'war')),\n",
       " (2.791021031612119e-05, ('mental', 'illness')),\n",
       " (2.7814864263462394e-05, ('bronze', 'age')),\n",
       " (2.7803521779425393e-05, ('babe', 'ruth')),\n",
       " (2.7484427323478516e-05, ('mortality', 'rate')),\n",
       " (2.748061635096673e-05, ('diplomatic', 'relations')),\n",
       " (2.7398274507645043e-05, ('carried', 'out')),\n",
       " (2.730974411670084e-05, ('once', 'again')),\n",
       " (2.7193015788573708e-05, ('her', 'husband')),\n",
       " (2.7176831456494805e-05, ('national', 'anthem')),\n",
       " (2.7164723712214024e-05, ('e', 'g')),\n",
       " (2.6942488454733404e-05, ('role', 'playing')),\n",
       " (2.691312147006904e-05, ('right', 'wing')),\n",
       " (2.6824969188248973e-05, ('leap', 'years')),\n",
       " (2.679394006460142e-05, ('university', 'press')),\n",
       " (2.6751683516883323e-05, ('peace', 'prize')),\n",
       " (2.6732038794812575e-05, ('de', 'la')),\n",
       " (2.6666666666666667e-05, ('margaret', 'thatcher')),\n",
       " (2.6376072681905883e-05, ('new', 'orleans')),\n",
       " (2.6308153593190832e-05, ('austria', 'hungary')),\n",
       " (2.6242283182910884e-05, ('both', 'sides')),\n",
       " (2.603492160736333e-05, ('natural', 'resources')),\n",
       " (2.5966538898589946e-05, ('births', 'living')),\n",
       " (2.592736633516677e-05, ('singer', 'songwriter')),\n",
       " (2.57912441070915e-05, ('head', 'coach')),\n",
       " (2.573801652718209e-05, ('box', 'office')),\n",
       " (2.5720164609053497e-05, ('addison', 'wesley')),\n",
       " (2.5422403003754694e-05, ('latin', 'alphabet')),\n",
       " (2.5404752140350367e-05, ('football', 'league')),\n",
       " (2.5197460871250667e-05, ('space', 'shuttle')),\n",
       " (2.5128808453909267e-05, ('took', 'place')),\n",
       " (2.5123542778439514e-05, ('comic', 'book')),\n",
       " (2.510338410353305e-05, ('mathbf', 'mathbf')),\n",
       " (2.5083102708138988e-05, ('open', 'source')),\n",
       " (2.497888881010243e-05, ('east', 'timor')),\n",
       " (2.496010631518305e-05, ('e', 'mail')),\n",
       " (2.4920696804834395e-05, ('left', 'wing')),\n",
       " (2.49011615434123e-05, ('lesser', 'extent')),\n",
       " (2.486216674939601e-05, ('est', 'gdp')),\n",
       " (2.475949955337672e-05, ('day', 'saints')),\n",
       " (2.4686338732530225e-05, ('middle', 'east')),\n",
       " (2.456315375778463e-05, ('great', 'britain')),\n",
       " (2.435191097518967e-05, ('european', 'union')),\n",
       " (2.4341244329942247e-05, ('security', 'council')),\n",
       " (2.4210483394156863e-05, ('m', 'ori')),\n",
       " (2.4150042232881232e-05, ('pointed', 'out')),\n",
       " (2.4112212449810428e-05, ('industrial', 'revolution')),\n",
       " (2.411045804512406e-05, ('basketball', 'player')),\n",
       " (2.3953388986307787e-05, ('roman', 'catholics')),\n",
       " (2.3941200411788646e-05, ('heavily', 'influenced')),\n",
       " (2.390394438986377e-05, ('windows', 'xp')),\n",
       " (2.3843945143866835e-05, ('led', 'zeppelin')),\n",
       " (2.380413953986598e-05, ('james', 'bond')),\n",
       " (2.3703703703703703e-05, ('programming', 'language')),\n",
       " (2.346581493470224e-05, ('joseph', 'smith')),\n",
       " (2.3440015454955244e-05, ('remaining', 'events')),\n",
       " (2.3410742490446948e-05, ('st', 'louis')),\n",
       " (2.334983433024154e-05, ('short', 'lived')),\n",
       " (2.3343668435379567e-05, ('civil', 'rights')),\n",
       " (2.3169601482854495e-05, ('dia', 'britannica')),\n",
       " (2.310034327110101e-05, ('os', 'x')),\n",
       " (2.304338632047457e-05, ('divided', 'into')),\n",
       " (2.279898914981857e-05, ('machine', 'guns')),\n",
       " (2.2543380775979724e-05, ('th', 'anniversary')),\n",
       " (2.243516854000324e-05, ('programming', 'languages')),\n",
       " (2.2185110581881778e-05, ('greek', 'mythology')),\n",
       " (2.206287920573635e-05, ('great', 'deal')),\n",
       " (2.2006039085869136e-05, ('cayman', 'islands')),\n",
       " (2.188571599386773e-05, ('almost', 'exclusively')),\n",
       " (2.1816870348710816e-05, ('labour', 'party')),\n",
       " (2.1779137763935926e-05, ('pulitzer', 'prize')),\n",
       " (2.17173094901994e-05, ('kansas', 'city')),\n",
       " (2.1713826867161672e-05, ('orthodox', 'church')),\n",
       " (2.1654964236498456e-05, ('carbon', 'atoms')),\n",
       " (2.1631419263524767e-05, ('oxford', 'university')),\n",
       " (2.1247000907851212e-05, ('i', 'am')),\n",
       " (2.1201209656218146e-05, ('short', 'stories')),\n",
       " (2.115701702221597e-05, ('distinction', 'between')),\n",
       " (2.1055098446316824e-05, ('machine', 'gun')),\n",
       " (2.1046373052223944e-05, ('karl', 'marx')),\n",
       " (2.0989313281669027e-05, ('war', 'ii')),\n",
       " (2.090648553615543e-05, ('south', 'carolina')),\n",
       " (2.0903756048068443e-05, ('special', 'relativity')),\n",
       " (2.090093102836905e-05, ('d', 'ivoire')),\n",
       " (2.0717052041439847e-05, ('x', 'y')),\n",
       " (2.0669456743769733e-05, ('democratic', 'republic')),\n",
       " (2.0654259021384232e-05, ('picked', 'up')),\n",
       " (2.0347663305650078e-05, ('charles', 'darwin')),\n",
       " (2.0319084888621312e-05, ('federal', 'reserve')),\n",
       " (2.019473494410386e-05, ('judicial', 'branch')),\n",
       " (2.015203772031551e-05, ('open', 'quarter')),\n",
       " (2.014496865750173e-05, ('race', 'car')),\n",
       " (1.9981620822013064e-05, ('united', 'nations')),\n",
       " (1.996937566007469e-05, ('f', 'kennedy')),\n",
       " (1.9933257490775098e-05, ('american', 'actress')),\n",
       " (1.9884131190725406e-05, ('domestic', 'violence')),\n",
       " (1.9751135690302193e-05, ('asia', 'minor')),\n",
       " (1.9739316940676586e-05, ('red', 'cross')),\n",
       " (1.9699664815285392e-05, ('shortly', 'after')),\n",
       " (1.959775605693148e-05, ('attorney', 'general')),\n",
       " (1.9584708259790756e-05, ('mississippi', 'river')),\n",
       " (1.9557133776010264e-05, ('large', 'scale')),\n",
       " (1.944287605244288e-05, ('stock', 'exchange')),\n",
       " (1.9347870951584815e-05, ('football', 'player')),\n",
       " (1.9067473432653685e-05, ('immune', 'system')),\n",
       " (1.9029185333391327e-05, ('nervous', 'system')),\n",
       " (1.901101567094451e-05, ('democratic', 'party')),\n",
       " (1.8947827157920666e-05, ('electricity', 'production')),\n",
       " (1.8749062546872656e-05, ('legislative', 'branch')),\n",
       " (1.8680415452439663e-05, ('law', 'enforcement')),\n",
       " (1.8614226767441005e-05, ('video', 'game')),\n",
       " (1.8545011391935568e-05, ('morse', 'code')),\n",
       " (1.8531890680330663e-05, ('difference', 'between')),\n",
       " (1.8447503193812005e-05, ('ph', 'd')),\n",
       " (1.8391450316332946e-05, ('mediterranean', 'sea')),\n",
       " (1.826630815992518e-05, ('tennis', 'player')),\n",
       " (1.8230262235310615e-05, ('new', 'jersey')),\n",
       " (1.8218831935236013e-05, ('central', 'asia')),\n",
       " (1.8127496926497754e-05, ('american', 'actor')),\n",
       " (1.7986668281469774e-05, ('harvard', 'university')),\n",
       " (1.7764243116355794e-05, ('great', 'lakes')),\n",
       " (1.7677875119410646e-05, ('chief', 'justice')),\n",
       " (1.7601498690166872e-05, ('south', 'wales')),\n",
       " (1.7579282564365287e-05, ('bbc', 'news')),\n",
       " (1.755226760034555e-05, ('national', 'assembly')),\n",
       " (1.7281144277614937e-05, ('african', 'americans')),\n",
       " (1.713195466975179e-05, ('new', 'testament')),\n",
       " (1.7122603548944963e-05, ('cape', 'breton')),\n",
       " (1.6945735160562276e-05, ('cold', 'war')),\n",
       " (1.6775196194906212e-05, ('best', 'selling')),\n",
       " (1.6739927552950863e-05, ('free', 'software')),\n",
       " (1.672558634583951e-05, ('counter', 'strike')),\n",
       " (1.6710943827071166e-05, ('roman', 'emperor')),\n",
       " (1.668704037875701e-05, ('british', 'columbia')),\n",
       " (1.6223335903366744e-05, ('total', 'population')),\n",
       " (1.6153293340981425e-05, ('th', 'centuries')),\n",
       " (1.60567684458602e-05, ('radio', 'stations')),\n",
       " (1.605205039831735e-05, ('high', 'school')),\n",
       " (1.6015203766775925e-05, ('dark', 'matter')),\n",
       " (1.5965833437365312e-05, ('present', 'day')),\n",
       " (1.5869608838350012e-05, ('st', 'earl')),\n",
       " (1.5852918733711124e-05, ('private', 'sector')),\n",
       " (1.5788808892257167e-05, ('episcopal', 'church')),\n",
       " (1.5707820888116604e-05, ('video', 'games')),\n",
       " (1.5605112935635436e-05, ('george', 'washington')),\n",
       " (1.5575442314070057e-05, ('cambridge', 'university')),\n",
       " (1.554179040198791e-05, ('does', 'not')),\n",
       " (1.544146589485541e-05, ('g', 'del')),\n",
       " (1.543108876749925e-05, ('american', 'musician')),\n",
       " (1.5426955625958033e-05, ('vietnam', 'war')),\n",
       " (1.54214594143474e-05, ('fan', 'fiction')),\n",
       " (1.5412237634459857e-05, ('great', 'depression')),\n",
       " (1.5403285520801587e-05, ('bah', 'faith')),\n",
       " (1.5356445937355385e-05, ('folk', 'music')),\n",
       " (1.524687743950039e-05, ('web', 'browser')),\n",
       " (1.5206645642071044e-05, ('air', 'pollution')),\n",
       " (1.5130197942108987e-05, ('at', 'least')),\n",
       " (1.507682192361126e-05, ('black', 'hole')),\n",
       " (1.4982930984009754e-05, ('tv', 'series')),\n",
       " (1.497349164742784e-05, ('frac', 'right')),\n",
       " (1.4913858129056498e-05, ('princeton', 'university')),\n",
       " (1.485736305890272e-05, ('latter', 'day')),\n",
       " (1.4804970142361176e-05, ('crown', 'prince')),\n",
       " (1.4739437917204043e-05, ('film', 'director')),\n",
       " (1.467768991133477e-05, ('golden', 'age')),\n",
       " (1.4672088510896518e-05, ('y', 'z')),\n",
       " (1.4648795136600015e-05, ('major', 'league')),\n",
       " (1.4629002964443242e-05, ('vector', 'space')),\n",
       " (1.4551191909423728e-05, ('governor', 'general')),\n",
       " (1.4405235648374745e-05, ('new', 'brunswick')),\n",
       " (1.4272864860206713e-05, ('non', 'profit')),\n",
       " (1.4231800373682135e-05, ('public', 'domain')),\n",
       " (1.4139525872793482e-05, ('real', 'numbers')),\n",
       " (1.3866163787126654e-05, ('classical', 'mechanics')),\n",
       " (1.3819317697571016e-05, ('solar', 'system')),\n",
       " (1.3744894074411917e-05, ('full', 'text')),\n",
       " (1.3700265589434355e-05, ('short', 'story')),\n",
       " (1.3504106695303947e-05, ('most', 'notably')),\n",
       " (1.3474051426779554e-05, ('un', 'security')),\n",
       " (1.346911434404128e-05, ('economic', 'growth')),\n",
       " (1.3424422768728583e-05, ('sea', 'level')),\n",
       " (1.3374603538537965e-05, ('minor', 'bwv')),\n",
       " (1.3324450366422385e-05, ('fertility', 'rate')),\n",
       " (1.331303096111763e-05, ('television', 'series')),\n",
       " (1.3270111501302737e-05, ('radio', 'broadcast')),\n",
       " (1.3238483432278825e-05, ('york', 'city')),\n",
       " (1.3183568000843748e-05, ('foreign', 'investment')),\n",
       " (1.3157833452103158e-05, ('official', 'site')),\n",
       " (1.3118798893154059e-05, ('did', 'not')),\n",
       " (1.3116595187131273e-05, ('broke', 'out')),\n",
       " (1.3089957966416108e-05, ('relationship', 'between')),\n",
       " (1.3020287053928583e-05, ('latin', 'america')),\n",
       " (1.2881131058220509e-05, ('west', 'coast')),\n",
       " (1.285069883842751e-05, ('australian', 'open')),\n",
       " (1.2843793466726963e-05, ('we', 'know')),\n",
       " (1.2843565373747753e-05, ('north', 'korea')),\n",
       " (1.2800107440163944e-05, ('world', 'war')),\n",
       " (1.2772472063520587e-05, ('decision', 'making')),\n",
       " (1.2688192406022156e-05, ('c', 'te')),\n",
       " (1.2687254478130933e-05, ('military', 'expenditures')),\n",
       " (1.2668710760173945e-05, ('recent', 'years')),\n",
       " (1.2596917534279362e-05, ('lung', 'cancer')),\n",
       " (1.2591373155825296e-05, ('community', 'college')),\n",
       " (1.2559492332099418e-05, ('mentioned', 'above')),\n",
       " (1.2418812016442507e-05, ('crude', 'oil')),\n",
       " (1.238225667373798e-05, ('new', 'hampshire')),\n",
       " (1.2254384562401871e-05, ('not', 'necessarily')),\n",
       " (1.2241323957470067e-05, ('even', 'though')),\n",
       " (1.22396707915073e-05, ('rock', 'band')),\n",
       " (1.2227848844223727e-05, ('feast', 'day')),\n",
       " (1.2173881410067742e-05, ('cia', 'world')),\n",
       " (1.212538808194493e-05, ('american', 'singer')),\n",
       " (1.2108967965406591e-05, ('aircraft', 'carrier')),\n",
       " (1.209141314146233e-05, ('east', 'asia')),\n",
       " (1.2090306184919598e-05, ('less', 'than'))]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta = 100  # You can play with this hyperparameter to see how it affects the results.\n",
    "unigrams, bigrams = word_utils.unigram_and_bigram_counts(words)\n",
    "scored_bigrams = sorted(\n",
    "    [(word_stream.score_bigram(bigram, unigrams, bigrams, delta), bigram) for bigram in bigrams],\n",
    "    reverse=True)\n",
    "scored_bigrams[0:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next cell uses the scores computed above and calls grouped_stream which takes a list of words and a set of n-grams and returns the list of words with those n-grams combined into single tokens.\n",
    "\n",
    "e.g. ['the', 'supreme', 'court'] => ['the', 'supreme_court']\n",
    "\n",
    "(You can find more examples in the tests for the function in word_stream_test.py.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# You can leave this cell alone.\n",
    "\n",
    "# If you want to come back afterwards, you can experiment with different phrase_thresholds.\n",
    "# You should set phrase_threshold to a value at which the bigrams in the previous\n",
    "# output start looking less tightly coupled.  grouped_stream below will re-tokenize\n",
    "# a stream of words to consider bigrams scoring above phrase_threshold as a single token.\n",
    "\n",
    "phrase_threshold = 1.0\n",
    "phrases = [bigram for score, bigram in scored_bigrams if score >= phrase_threshold]\n",
    "words = word_utils.grouped_stream(words, phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Cleanup some variables to recover some memory.\n",
    "del unigrams\n",
    "del bigrams\n",
    "del scored_bigrams\n",
    "del phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. TensorFlow for GloVe\n",
    "\n",
    "### Cooccurrence Table\n",
    "In this section, we first build the cooccurrence table with context window of size C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 20000  # Amount of vocabulary to keep.  Lower frequency words are mapped to <UNK> (word id: 0).\n",
    "\n",
    "# Map each of the words to a wordid.  Only the most popular VOCAB_SIZE words are kept.\n",
    "vocabulary = word_utils.Vocabulary(words, VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert the word stream to wordids.\n",
    "# We need to do this because the TensorFlow code you will write in the\n",
    "# next section will use an API that expects indexes into the embedding\n",
    "# matrix, not words.\n",
    "wordids = [vocabulary.to_id(word) for word in words]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Building the cooccurrence table takes a considerable amount of time\n",
    "# with the Wikipedia set.\n",
    "\n",
    "C = 10  # Context window size.\n",
    "ctable = word_stream.cooccurrence_table(wordids, C)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<UNK>', '<UNK>', 706138.1285682213),\n",
       " ('the', 'of', 374048.6742070548),\n",
       " ('of', 'the', 374048.57420705474),\n",
       " ('<UNK>', 'the', 342798.24642992363),\n",
       " ('the', '<UNK>', 342798.24642992363),\n",
       " ('the', 'the', 269496.53413024516),\n",
       " ('zero', 'zero', 264773.4888895979),\n",
       " ('one', 'nine', 257475.82539757458),\n",
       " ('nine', 'one', 257475.71428646345),\n",
       " ('<UNK>', 'and', 193557.81507994025)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Output top words occurring within the same context.\n",
    "# If everything has worked properly, you should see a considerable number of \"<UNK>\", \"the\", \"of\", and numbers.\n",
    "sorted([(vocabulary.to_word(word), vocabulary.to_word(context_word), count) for word, context_word, count in ctable if count > len(words) / 100], key=lambda x: x[2], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Shard the table into word lists, context word lists and corresponding counts.\n",
    "# We are going to provide these to TensorFlow as their own entry in the feed_dict,\n",
    "# so we do this separation once, up front.\n",
    "ctable_wids = np.array([word for word, _, _ in ctable])\n",
    "ctable_cwids = np.array([context_word for _, context_word, _ in ctable])\n",
    "ctable_counts = np.array([count for _, _, count in ctable])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0, 706138.12856822135),\n",
       " (0, 1, 107923.21904752152),\n",
       " (0, 2, 117.23730158730162),\n",
       " (0, 3, 29.944841269841259),\n",
       " (0, 4, 50.867857142857197)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here is what our final data looks like.  It's similar to the table two\n",
    "# cells previous, except instead of words, there are wordids.\n",
    "zip(ctable_wids[:5], ctable_cwids[:5], ctable_counts[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow Graph Setup (12 points)\n",
    "\n",
    "Complete the functions in **glove.py** using the TensorFlow API and then run the corresponding tests in the cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_embedding_lookup (glove_test.TestGlove) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.170s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=1 errors=0 failures=0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(glove)\n",
    "reload(glove_test)\n",
    "unittest.TextTestRunner(verbosity=2).run(\n",
    "    unittest.TestLoader().loadTestsFromName(\n",
    "        'TestGlove.test_embedding_lookup', glove_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_example_weight (glove_test.TestGlove) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.044s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=1 errors=0 failures=0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(glove)\n",
    "reload(glove_test)\n",
    "unittest.TextTestRunner(verbosity=2).run(\n",
    "    unittest.TestLoader().loadTestsFromName(\n",
    "        'TestGlove.test_example_weight', glove_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_loss (glove_test.TestGlove) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.095s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=1 errors=0 failures=0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(glove)\n",
    "reload(glove_test)\n",
    "unittest.TextTestRunner(verbosity=2).run(\n",
    "    unittest.TestLoader().loadTestsFromName(\n",
    "        'TestGlove.test_loss', glove_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Hyperparameters.\n",
    "\n",
    "# You may want to shrink num_examples_to_train to finish debugging\n",
    "# and only run it this long once you are training on Wikipedia.\n",
    "learning_rate = 0.003\n",
    "num_examples_to_train = 300000000\n",
    "#num_examples_to_train = 3000\n",
    "batch_size = 100\n",
    "embedding_dim = 300\n",
    "\n",
    "# Construct the training graph.\n",
    "tf.reset_default_graph()\n",
    "\n",
    "wids_ph = tf.placeholder(tf.int32, shape=[None])\n",
    "c_wids_ph = tf.placeholder(tf.int32, shape=[None])\n",
    "counts_ph = tf.placeholder(tf.float32, shape=[None])\n",
    "\n",
    "with tf.variable_scope('word_embeddings'):\n",
    "    word_embeddings, word_bias, word_embed_matrix = (\n",
    "        glove.wordids_to_tensors(wids_ph, embedding_dim, vocabulary.size()))\n",
    "with tf.variable_scope('word_context_embeddings'):\n",
    "    word_context_embeddings, word_context_bias, word_context_embed_matrix = (\n",
    "        glove.wordids_to_tensors(c_wids_ph, embedding_dim, vocabulary.size()))\n",
    "    \n",
    "losses = glove.loss(\n",
    "    word_embeddings, word_bias, word_context_embeddings, word_context_bias,\n",
    "    tf.cast(counts_ph, tf.float32))\n",
    "loss = tf.reduce_mean(losses)\n",
    "\n",
    "# Adam is similar to AdaGrad in that it handles sparse gradients well.\n",
    "# Specifically, you may imagine that some words appear with more context\n",
    "# words than others and with bigger counts.  They therefore are updated\n",
    "# more often and more aggressively (remember the weighting function\n",
    "# you implemented).  Adam backs off updating parameters that it has already\n",
    "# significantly moved around.  (intuitively: the 500th time you backprop\n",
    "# into \"the\", you probably don't have a lot more information to add).\n",
    "#\n",
    "# Here is the original University of Toronto paper detailing the word\n",
    "# done in collaboration with Google DeepMind.\n",
    "# https://arxiv.org/pdf/1412.6980v8.pdf\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train the embeddings.\n",
    "# Set up the session & initialize variables.\n",
    "sess = tf.Session()\n",
    "sess.run(tf.initialize_all_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training examples: 31302717\n",
      "# of epochs: 9.58383261108\n",
      "# batches: 3000001\n",
      "Initial loss: 0.13992\n",
      "0 : 0.143553\n",
      "Expected time left: 54.5590250133 hours ( 65.4926390648 seconds per 1000 batches).\n",
      "1000 : 0.141832\n",
      "Expected time left: 54.4925806536 hours ( 65.4346981049 seconds per 1000 batches).\n",
      "2000 : 0.128669\n",
      "Expected time left: 54.6783842879 hours ( 65.6797189713 seconds per 1000 batches).\n",
      "3000 : 0.137052\n",
      "Expected time left: 54.4333652111 hours ( 65.4072260857 seconds per 1000 batches).\n",
      "4000 : 0.123202\n",
      "Expected time left: 54.4790259142 hours ( 65.4839491844 seconds per 1000 batches).\n",
      "5000 : 0.137179\n",
      "Expected time left: 54.6199488141 hours ( 65.6752672195 seconds per 1000 batches).\n",
      "6000 : 0.156856\n",
      "Expected time left: 54.6549525585 hours ( 65.7393128872 seconds per 1000 batches).\n",
      "7000 : 0.119653\n",
      "Expected time left: 54.3579726572 hours ( 65.4039559364 seconds per 1000 batches).\n",
      "8000 : 0.153971\n",
      "Expected time left: 54.2010142326 hours ( 65.2369060516 seconds per 1000 batches).\n",
      "9000 : 0.13421\n",
      "Expected time left: 54.70044472 hours ( 65.8600451946 seconds per 1000 batches).\n",
      "10000 : 0.135991\n",
      "Expected time left: 54.1228233334 hours ( 65.186383009 seconds per 1000 batches).\n",
      "11000 : 0.128672\n",
      "Expected time left: 53.4981061616 hours ( 64.4555280209 seconds per 1000 batches).\n",
      "12000 : 0.145358\n",
      "Expected time left: 53.5413588841 hours ( 64.5292358398 seconds per 1000 batches).\n",
      "13000 : 0.130279\n",
      "Expected time left: 53.4344929797 hours ( 64.4220061302 seconds per 1000 batches).\n",
      "14000 : 0.135451\n",
      "Expected time left: 53.6500125104 hours ( 64.7035109997 seconds per 1000 batches).\n",
      "15000 : 0.133959\n",
      "Expected time left: 53.3241308872 hours ( 64.3320398331 seconds per 1000 batches).\n",
      "16000 : 0.130714\n",
      "Expected time left: 53.4060349327 hours ( 64.4524509907 seconds per 1000 batches).\n",
      "17000 : 0.129294\n",
      "Expected time left: 53.6740556986 hours ( 64.7976310253 seconds per 1000 batches).\n",
      "18000 : 0.126079\n",
      "Expected time left: 53.6909166784 hours ( 64.8397300243 seconds per 1000 batches).\n",
      "19000 : 0.150661\n",
      "Expected time left: 53.3455409791 hours ( 64.4442560673 seconds per 1000 batches).\n",
      "20000 : 0.134131\n",
      "Expected time left: 53.2935799945 hours ( 64.403096199 seconds per 1000 batches).\n",
      "21000 : 0.12966\n",
      "Expected time left: 53.2672068002 hours ( 64.3928408623 seconds per 1000 batches).\n",
      "22000 : 0.125207\n",
      "Expected time left: 53.5363847632 hours ( 64.7399799824 seconds per 1000 batches).\n",
      "23000 : 0.121657\n",
      "Expected time left: 53.4222113002 hours ( 64.6236209869 seconds per 1000 batches).\n",
      "24000 : 0.130245\n",
      "Expected time left: 53.3235799002 hours ( 64.525990963 seconds per 1000 batches).\n",
      "25000 : 0.148316\n",
      "Expected time left: 53.1341173767 hours ( 64.3183450699 seconds per 1000 batches).\n",
      "26000 : 0.126162\n",
      "Expected time left: 53.292201623 hours ( 64.5314030647 seconds per 1000 batches).\n",
      "27000 : 0.134519\n",
      "Expected time left: 53.5967879144 hours ( 64.9220631123 seconds per 1000 batches).\n",
      "28000 : 0.136641\n",
      "Expected time left: 47.7577005502 hours ( 57.8686180115 seconds per 1000 batches).\n",
      "29000 : 0.131546\n",
      "Expected time left: 47.3944782332 hours ( 57.4478330612 seconds per 1000 batches).\n",
      "30000 : 0.149073\n",
      "Expected time left: 47.2385626139 hours ( 57.2781300545 seconds per 1000 batches).\n",
      "31000 : 0.131768\n",
      "Expected time left: 46.7846368687 hours ( 56.7468450069 seconds per 1000 batches).\n",
      "32000 : 0.124432\n",
      "Expected time left: 47.5473619961 hours ( 57.6914207935 seconds per 1000 batches).\n",
      "33000 : 0.133343\n",
      "Expected time left: 47.5283691261 hours ( 57.6878190041 seconds per 1000 batches).\n",
      "34000 : 0.133613\n",
      "Expected time left: 47.4446809471 hours ( 57.6056640148 seconds per 1000 batches).\n",
      "35000 : 0.123245\n",
      "Expected time left: 47.5087962553 hours ( 57.7029719353 seconds per 1000 batches).\n",
      "36000 : 0.133479\n",
      "Expected time left: 46.6387073901 hours ( 56.6653020382 seconds per 1000 batches).\n",
      "37000 : 0.128096\n",
      "Expected time left: 46.9988975592 hours ( 57.1222059727 seconds per 1000 batches).\n",
      "38000 : 0.122516\n",
      "Expected time left: 47.1135149818 hours ( 57.2808499336 seconds per 1000 batches).\n",
      "39000 : 0.145731\n",
      "Expected time left: 47.1419059435 hours ( 57.334731102 seconds per 1000 batches).\n",
      "40000 : 0.151137\n",
      "Expected time left: 47.0914437959 hours ( 57.2927138805 seconds per 1000 batches).\n",
      "41000 : 0.124468\n",
      "Expected time left: 47.1137646532 hours ( 57.339247942 seconds per 1000 batches).\n",
      "42000 : 0.130293\n",
      "Expected time left: 46.9559840175 hours ( 57.1665489674 seconds per 1000 batches).\n",
      "43000 : 0.153407\n",
      "Expected time left: 47.1123360065 hours ( 57.376303196 seconds per 1000 batches).\n",
      "44000 : 0.124261\n",
      "Expected time left: 47.2520322994 hours ( 57.5659081936 seconds per 1000 batches).\n",
      "45000 : 0.133122\n",
      "Expected time left: 47.554536193 hours ( 57.9540529251 seconds per 1000 batches).\n",
      "46000 : 0.127532\n",
      "Expected time left: 47.1536702477 hours ( 57.4849832058 seconds per 1000 batches).\n",
      "47000 : 0.129266\n",
      "Expected time left: 46.8627420531 hours ( 57.1496660709 seconds per 1000 batches).\n",
      "48000 : 0.141259\n",
      "Expected time left: 46.7255554454 hours ( 57.0016748905 seconds per 1000 batches).\n",
      "49000 : 0.162612\n",
      "Expected time left: 46.985119127 hours ( 57.3377530575 seconds per 1000 batches).\n",
      "50000 : 0.136727\n",
      "Expected time left: 46.8730298382 hours ( 57.2203629017 seconds per 1000 batches).\n",
      "51000 : 0.178817\n",
      "Expected time left: 46.2263672975 hours ( 56.4500901699 seconds per 1000 batches).\n",
      "52000 : 0.156992\n",
      "Expected time left: 46.7228042281 hours ( 57.0756831169 seconds per 1000 batches).\n",
      "53000 : 0.12229\n",
      "Expected time left: 46.429423383 hours ( 56.7365469933 seconds per 1000 batches).\n",
      "54000 : 0.138117\n",
      "Expected time left: 47.0002084896 hours ( 57.4535460472 seconds per 1000 batches).\n",
      "55000 : 0.138319\n",
      "Expected time left: 46.2909891752 hours ( 56.605809927 seconds per 1000 batches).\n",
      "56000 : 0.167336\n",
      "Expected time left: 46.2244773813 hours ( 56.5436840057 seconds per 1000 batches).\n",
      "57000 : 0.14418\n",
      "Expected time left: 46.4026144384 hours ( 56.7808821201 seconds per 1000 batches).\n",
      "58000 : 0.124434\n",
      "Expected time left: 46.6735633317 hours ( 57.1318500042 seconds per 1000 batches).\n",
      "59000 : 0.127884\n",
      "Expected time left: 46.4364010448 hours ( 56.8608798981 seconds per 1000 batches).\n",
      "60000 : 0.149968\n",
      "Expected time left: 46.7622155448 hours ( 57.2793190479 seconds per 1000 batches).\n",
      "61000 : 0.131769\n",
      "Expected time left: 42.5051507772 hours ( 52.0825359821 seconds per 1000 batches).\n",
      "62000 : 0.139554\n",
      "Expected time left: 44.3826204224 hours ( 54.4015591145 seconds per 1000 batches).\n",
      "63000 : 0.166268\n",
      "Expected time left: 42.9123712543 hours ( 52.617331028 seconds per 1000 batches).\n",
      "64000 : 0.140763\n",
      "Expected time left: 46.9258134631 hours ( 57.5580480099 seconds per 1000 batches).\n",
      "65000 : 0.122418\n",
      "Expected time left: 47.6471485171 hours ( 58.4627389908 seconds per 1000 batches).\n",
      "66000 : 0.158762\n",
      "Expected time left: 47.5885238356 hours ( 58.4107151031 seconds per 1000 batches).\n",
      "67000 : 0.135669\n",
      "Expected time left: 47.5567794827 hours ( 58.3916602135 seconds per 1000 batches).\n",
      "68000 : 0.121953\n",
      "Expected time left: 47.3417316695 hours ( 58.1474499702 seconds per 1000 batches).\n",
      "69000 : 0.12537\n",
      "Expected time left: 48.997103731 hours ( 60.2011990547 seconds per 1000 batches).\n",
      "70000 : 0.115648\n",
      "Expected time left: 49.3586966291 hours ( 60.6661820412 seconds per 1000 batches).\n",
      "71000 : 0.127803\n",
      "Expected time left: 47.5367354557 hours ( 58.4467859268 seconds per 1000 batches).\n",
      "72000 : 0.140725\n",
      "Expected time left: 45.1571884113 hours ( 55.5400829315 seconds per 1000 batches).\n",
      "73000 : 0.128542\n",
      "Expected time left: 46.6438613115 hours ( 57.3881897926 seconds per 1000 batches).\n",
      "74000 : 0.116026\n",
      "Expected time left: 46.8506445423 hours ( 57.6623120308 seconds per 1000 batches).\n",
      "75000 : 0.129863\n",
      "Expected time left: 46.175128483 hours ( 56.8503439426 seconds per 1000 batches).\n",
      "76000 : 0.13009\n",
      "Expected time left: 45.8325882761 hours ( 56.4479169846 seconds per 1000 batches).\n",
      "77000 : 0.139937\n",
      "Expected time left: 47.4139425244 hours ( 58.415514946 seconds per 1000 batches).\n",
      "78000 : 0.130613\n",
      "Expected time left: 49.2908623328 hours ( 60.7487311363 seconds per 1000 batches).\n",
      "79000 : 0.128079\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-8f3e48a6b4d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;31m# Train based on randomly sampled batches of examples.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mloss_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_op\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmake_feed_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;31m# Do some basic reporting as training progresses.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    708\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 710\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    711\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    906\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 908\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    909\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    956\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m--> 958\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m    959\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    960\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m    963\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m    945\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m    946\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m    948\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Important note:  You do not need to run this cell to completion.\n",
    "# Let it train for 30 minutes or so, then interrupt the kernel and see how good\n",
    "# the nearest-neighbors results look.  Run this cell again to pick up from where\n",
    "# you left off.\n",
    "\n",
    "# An hour on the recommended GCE cloud instance gets reasonably good results.\n",
    "# Two hours cleans up the vectors beautifully.\n",
    "\n",
    "REPORT_LOSS_EVERY = 1000\n",
    "EVAL_BATCH_SIZE = 5000\n",
    "\n",
    "indexes = range(len(ctable_wids))\n",
    "\n",
    "def make_feed_dict(feed_dict_batch_size):\n",
    "    batch_idx = random.sample(indexes, feed_dict_batch_size)\n",
    "    batch_wids = ctable_wids[batch_idx]\n",
    "    batch_cwids = ctable_cwids[batch_idx]\n",
    "    batch_counts = ctable_counts[batch_idx]\n",
    "    return {\n",
    "        wids_ph: batch_wids,\n",
    "        c_wids_ph: batch_cwids,\n",
    "        counts_ph: batch_counts\n",
    "    }\n",
    "\n",
    "num_batches = num_examples_to_train / batch_size + 1\n",
    "\n",
    "print '# training examples:', len(ctable_wids)\n",
    "print '# of epochs:', 1.0 * num_examples_to_train / len(ctable_wids)\n",
    "print '# batches:', num_batches\n",
    "print 'Initial loss:', sess.run(loss, feed_dict=make_feed_dict(EVAL_BATCH_SIZE))\n",
    "\n",
    "current_timer = None\n",
    "for batch in xrange(num_batches):\n",
    "    # Train based on randomly sampled batches of examples.\n",
    "    loss_val, _ = sess.run([loss, train_op], feed_dict=make_feed_dict(batch_size))\n",
    "    \n",
    "    # Do some basic reporting as training progresses.\n",
    "    if batch % REPORT_LOSS_EVERY == 0:\n",
    "        if current_timer:\n",
    "            remaining_reporting_cycles = 1.0 * (num_batches - batch) / REPORT_LOSS_EVERY\n",
    "            cycle_time = time.time() - current_timer\n",
    "            print 'Expected time left:', remaining_reporting_cycles * cycle_time / 60 / 60, 'hours (', cycle_time, 'seconds per', REPORT_LOSS_EVERY, 'batches).'\n",
    "        current_timer = time.time()\n",
    "            \n",
    "        print batch, ':', sess.run(loss, feed_dict=make_feed_dict(EVAL_BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play! (1 point)\n",
    "\n",
    "Congratulations!  You now have some embeddings.  We only trained a short while over not a lot of text, but these still work reasonably well.\n",
    "\n",
    "If you want more compelling vectors, scroll back up to the top of this notebook and follow the instructions to switch the data source to Wikipedia and execute it again.  Note:  training these vectors for a long time is **not required**, but since you've gotten this far, it takes almost no additional effort to see the result of your hard work below.  The longer you run on the Wikipedia set, the nicer your word vectors will be.  As noted in the previous cell, you can let it run for a while, interrupt the kernel, see how things look, and then run that cell again to have it pick up from where it left off.\n",
    "\n",
    "We have a number of suggestions below to get you started exploring the space.  Feel free to try some of your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_nn_cos(v, Wv, k=10):\n",
    "    \"\"\"Find nearest neighbors, by cosine distance.\"\"\"\n",
    "    Z = np.linalg.norm(Wv, axis=1) * np.linalg.norm(v)\n",
    "    ds = np.dot(Wv, v.T) / Z\n",
    "    nns = np.argsort(-1*ds)[:k]  # sort descending, take best\n",
    "    return nns, ds[nns]  # word indices, distances\n",
    "\n",
    "def show_nns(v, Wv, vocabulary, k=10):\n",
    "    print \"Nearest neighbors:\"\n",
    "    for i, d in zip(*find_nn_cos(v, Wv, k)):\n",
    "        w = vocabulary.to_word(i)\n",
    "        print \"%.03f : \\\"%s\\\"\" % (d, w)\n",
    "        \n",
    "def word_show_nns(word, Wv, vocabulary, k=10):\n",
    "    show_nns(Wv[vocabulary.to_id(word)], Wv, vocabulary, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_embed_matrix_val, word_context_embed_matrix_val = sess.run([word_embed_matrix, word_context_embed_matrix])\n",
    "\n",
    "# As per the paper, we take the average of the word's vector when it's the center word of the window\n",
    "# and the vector when it's found in the context.\n",
    "#\n",
    "# There is some (handwave-y) motivation for why we do this in section 4.2 of GloVe.\n",
    "Wv = word_embed_matrix_val + word_context_embed_matrix_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest neighbors:\n",
      "1.000 : \"one\"\n",
      "0.291 : \"nine\"\n",
      "0.283 : \"four\"\n",
      "0.283 : \"eight\"\n",
      "0.257 : \"b\"\n",
      "0.241 : \"two\"\n",
      "0.240 : \"seven\"\n",
      "0.233 : \"zero\"\n",
      "0.228 : \"january\"\n",
      "0.214 : \"fourth\"\n"
     ]
    }
   ],
   "source": [
    "word_show_nns('one', Wv, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest neighbors:\n",
      "1.000 : \"king\"\n",
      "0.350 : \"reign\"\n",
      "0.271 : \"john\"\n",
      "0.254 : \"hungary\"\n",
      "0.234 : \"baptiste\"\n",
      "0.229 : \"lovelace\"\n",
      "0.224 : \"bishop\"\n",
      "0.216 : \"nat\"\n",
      "0.213 : \"saul\"\n",
      "0.210 : \"died\"\n"
     ]
    }
   ],
   "source": [
    "word_show_nns('king', Wv, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest neighbors:\n",
      "1.000 : \"car\"\n",
      "0.309 : \"racing\"\n",
      "0.262 : \"rent\"\n",
      "0.259 : \"concave\"\n",
      "0.243 : \"handbook\"\n",
      "0.240 : \"bomb\"\n",
      "0.230 : \"driver\"\n",
      "0.228 : \"temptation\"\n",
      "0.228 : \"motor\"\n",
      "0.226 : \"company\"\n"
     ]
    }
   ],
   "source": [
    "word_show_nns('car', Wv, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest neighbors:\n",
      "1.000 : \"computer\"\n",
      "0.338 : \"electronics\"\n",
      "0.328 : \"hardware\"\n",
      "0.291 : \"vision\"\n",
      "0.254 : \"database\"\n",
      "0.248 : \"algorithms\"\n",
      "0.241 : \"products\"\n",
      "0.239 : \"compatibility\"\n",
      "0.235 : \"insulation\"\n",
      "0.234 : \"rubin\"\n"
     ]
    }
   ],
   "source": [
    "word_show_nns('computer', Wv, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest neighbors:\n",
      "1.000 : \"college\"\n",
      "0.258 : \"crick\"\n",
      "0.258 : \"wesleyan\"\n",
      "0.250 : \"hill\"\n",
      "0.250 : \"renamed\"\n",
      "0.244 : \"gerry\"\n",
      "0.239 : \"league\"\n",
      "0.230 : \"david\"\n",
      "0.229 : \"albania\"\n",
      "0.228 : \"university\"\n"
     ]
    }
   ],
   "source": [
    "word_show_nns('college', Wv, vocabulary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
