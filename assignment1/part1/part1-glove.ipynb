{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GloVe: Global Vectors for Word Representation (25 points)\n",
    "\n",
    "[GloVe](http://nlp.stanford.edu/projects/glove/) \\[[PDF](http://nlp.stanford.edu/pubs/glove.pdf)\\] is (yet) another way to train word vectors.  Its main advantage relative to Word2Vec is its training speed.\n",
    "\n",
    "## Approach\n",
    "The intuition of the GloVe approach to training word vectors is as follows:\n",
    "\n",
    "1. From the training data, estimate $P(k | word)$.\n",
    "2. Notice that some words, $k$, are far more common than others in the context of $word$.\n",
    "3. In particular, in the table below, notice in the bottom row that $k$'s that are related to ice (vs. steam) result in quite large numbers where as those related to steam (vs. ice) are incredibly low.  Unrelated numbers are about 1.0.\n",
    "\n",
    "<img src=\"glove_table.png\">\n",
    "\n",
    "At a high level then training $F(w_i, w_j, \\tilde{w}_k) = \\frac{P_{ij}}{P_{jk}}$ seems like a useful thing to do.  In this case, F is a (simple) neural network accepting word vectors $w_i$ and $w_j$ for words $i$ and $j$, and a context vector $\\tilde{w}_k$ for word $k$.\n",
    "\n",
    "With some reasonable assumptions about desireable properties of vector embeddings (see Section 3 of the paper), the authors make this more concrete and simplify to a simple objective function based directly on the cooccurrence matrix instead of probabilities:\n",
    "\n",
    "$$J = \\sum\\limits_{i,j}^V f(X_{ij})(w_i^T\\tilde{w}_j + b_i + \\tilde{b}_j - log(X_{ij}))^2$$\n",
    "\n",
    "where $f(.)$ is the weight of the $j$'th word appearing in the $i$th word's context window $X_{ij}$ times.  This weighting function is described in detail immediately before Equation (9) in the paper.\n",
    "\n",
    "Note that $f(0) = 0$ pairs $i,j$ where $X_{ij} = 0$ can be skipped in the sum above.\n",
    "\n",
    "## vs. Word2Vec\n",
    "Similar to Word2Vec, GloVe embeds words in a vector space based on the \"[company it keeps](https://en.wikipedia.org/wiki/John_Rupert_Firth)\" - based on cooccurrance between words in small context windows.  Unlike Word2Vec which repeatedly iterates over the training data one context window at a time, GloVe does a single pass over the training data to collect cooccurrance statistics.  GloVe then trains entirely based on this table of counts.\n",
    "\n",
    "## The Plan\n",
    "In this assignment, you are going to train GloVe models and visualize them.\n",
    "\n",
    "1. Parsing utilities.\n",
    "\n",
    "2. Phrases.  Implement Section 4 (Equation 6) of the [Word2Vec](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) paper.  This allows us to treat \"los angeles\" as a single item in our vocabulary.\n",
    "\n",
    "3. Implement TensorFlow for GloVe & Train embeddings.\n",
    "\n",
    "5. Visualize embeddings.\n",
    "\n",
    "\n",
    "As usual, we begin by importing some useful libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'word_utils' from 'word_utils.pyc'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glove\n",
    "import glove_test\n",
    "import nltk\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import unittest\n",
    "import word_stream\n",
    "import word_stream_test\n",
    "import word_utils\n",
    "reload(glove)\n",
    "reload(glove_test)\n",
    "reload(word_stream)\n",
    "reload(word_stream_test)\n",
    "reload(word_utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Lower casing sometimes causes more harm than good,\n",
    "# but we do it here anyways in absence of more careful normalization.\n",
    "\n",
    "# The Brown corpus is only 1m tokens.  Test your code with this, then if you want, run with Wikipedia.\n",
    "\n",
    "# words = [w.lower() for w in nltk.corpus.brown.words()]\n",
    "\n",
    "# Use first billion bytes of Wikipedia, consisting of 17m tokens.  While this produces better\n",
    "# embeddings, all of the code runs correspondingly longer.  We recommend getting everything to work\n",
    "# with the Brown corpus before trying this.\n",
    "\n",
    "# If you are going to try this using Google Compute Engine, we recommend using the\n",
    "# n1-highcpu-16 (16 vCPUs, 14.4 GB memory) version.\n",
    "\n",
    "# You should not spend time debugging on this instance though or you will find yourself\n",
    "# without GCE credit!\n",
    "\n",
    "words = open('text8').read().split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['anarchism',\n",
       "  'originated',\n",
       "  'as',\n",
       "  'a',\n",
       "  'term',\n",
       "  'of',\n",
       "  'abuse',\n",
       "  'first',\n",
       "  'used',\n",
       "  'against'],\n",
       " 17005207)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[:10], len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  Parsing Utilities (8 points)\n",
    "\n",
    "In **word_stream.py** (find this in the same directory as this notebook), implement:\n",
    "1.  context_windows\n",
    "2.  cooccurrence_table\n",
    "\n",
    "You may find it helpful to run the follow unit tests to check your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_context_windows (word_stream_test.TestWordStreams) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.202s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=1 errors=0 failures=0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(word_stream)\n",
    "reload(word_stream_test)\n",
    "unittest.TextTestRunner(verbosity=2).run(\n",
    "    unittest.TestLoader().loadTestsFromName(\n",
    "        'TestWordStreams.test_context_windows', word_stream_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_cooccurrence_table (word_stream_test.TestWordStreams) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.001s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=1 errors=0 failures=0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(word_stream)\n",
    "reload(word_stream_test)\n",
    "unittest.TextTestRunner(verbosity=2).run(\n",
    "    unittest.TestLoader().loadTestsFromName(\n",
    "        'TestWordStreams.test_cooccurrence_table', word_stream_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  Phrases (4 points)\n",
    "\n",
    "Implement the function **score_bigram** in **word_stream.py**.\n",
    "Specifically, read the introduction to Section 4 of the [Word2Vec](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) paper and implement Equation 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_score_bigram (word_stream_test.TestWordStreams) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.001s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=1 errors=0 failures=0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(word_stream)\n",
    "reload(word_stream_test)\n",
    "unittest.TextTestRunner(verbosity=2).run(\n",
    "    unittest.TestLoader().loadTestsFromName(\n",
    "        'TestWordStreams.test_score_bigram', word_stream_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use your scoring code to see the best scoring phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.001957730812013348, ('burkina', 'faso')),\n",
       " (0.0018769551616266945, ('buenos', 'aires')),\n",
       " (0.0015462384775498067, ('costa', 'rica')),\n",
       " (0.001476912716754159, ('fran', 'ois')),\n",
       " (0.0014698871762275545, ('sri', 'lanka')),\n",
       " (0.0013311013855555331, ('las', 'vegas')),\n",
       " (0.00129174343010821, ('coca', 'cola')),\n",
       " (0.0012493492972410203, ('nova', 'scotia')),\n",
       " (0.0012483640390617136, ('saudi', 'arabia')),\n",
       " (0.0011100906440921615, ('monty', 'python')),\n",
       " (0.0010555160864711254, ('puerto', 'rico')),\n",
       " (0.001049632628579997, ('guant', 'namo')),\n",
       " (0.0010233918128654971, ('gda', 'sk')),\n",
       " (0.0008804754567466431, ('mario', 'kart')),\n",
       " (0.0008553519070943142, ('hip', 'hop')),\n",
       " (0.0007207648654192401, ('import', 'insubstantial')),\n",
       " (0.0007022071041776631, ('los', 'angeles')),\n",
       " (0.0006783348664121336, ('hong', 'kong')),\n",
       " (0.000629765412383887, ('walt', 'disney')),\n",
       " (0.0006038647342995169, ('ann', 'arbor')),\n",
       " (0.0005259159703149653, ('bene', 'gesserit')),\n",
       " (0.0005191225884396119, ('ronald', 'reagan')),\n",
       " (0.0005115641661034767, ('amino', 'acids')),\n",
       " (0.0005074865493609262, ('vice', 'versa')),\n",
       " (0.00048339984366643356, ('duplicate', 'info')),\n",
       " (0.00048192771084337347, ('encyclop', 'dia')),\n",
       " (0.00047006432459178625, ('bj', 'rk')),\n",
       " (0.0004453692549191399, ('anglo', 'saxon')),\n",
       " (0.00044122059264750007, ('infant', 'mortality')),\n",
       " (0.00041948120045649426, ('gregorian', 'calendar')),\n",
       " (0.00041688859079820357, ('carbon', 'dioxide')),\n",
       " (0.0004055895234111655, ('http', 'www')),\n",
       " (0.000374960279631395, ('mac', 'os')),\n",
       " (0.00035818535886467595, ('equatorial', 'guinea')),\n",
       " (0.00035357376349250824, ('bipolar', 'disorder')),\n",
       " (0.0003502155172413793, ('el', 'salvador')),\n",
       " (0.0003448019417081894, ('prize', 'laureate')),\n",
       " (0.0003316664088948463, ('super', 'bowl')),\n",
       " (0.0003305300748292829, ('adolf', 'hitler')),\n",
       " (0.00032976617776752455, ('san', 'francisco')),\n",
       " (0.00032346149356066753, ('nobel', 'prize')),\n",
       " (0.0003231633549327986, ('cape', 'verde')),\n",
       " (0.00032004963714617825, ('quarter', 'finalist')),\n",
       " (0.00031386917420379715, ('global', 'warming')),\n",
       " (0.00031242734247849337, ('lindy', 'hop')),\n",
       " (0.00030902348578491963, ('julius', 'caesar')),\n",
       " (0.00028796866900881183, ('winston', 'churchill')),\n",
       " (0.00027368609376464273, ('quantum', 'mechanics')),\n",
       " (0.0002558617936935185, ('von', 'neumann')),\n",
       " (0.0002527805864509606, ('bin', 'laden')),\n",
       " (0.0002519196275620226, ('grand', 'prix')),\n",
       " (0.00024368976769511365, ('martial', 'arts')),\n",
       " (0.00023923444976076556, ('schr', 'dinger')),\n",
       " (0.00023570059532042578, ('per', 'capita')),\n",
       " (0.0002338946805666934, ('elvis', 'presley')),\n",
       " (0.000231000231000231, ('avant', 'garde')),\n",
       " (0.00023056881326231814, ('blade', 'runner')),\n",
       " (0.0002249799159560005, ('star', 'trek')),\n",
       " (0.0002247765006385696, ('gaza', 'strip')),\n",
       " (0.00022148394241417498, ('khmer', 'rouge')),\n",
       " (0.00021824530772588389, ('anti', 'semitism')),\n",
       " (0.0002161953301808681, ('ms', 'dos')),\n",
       " (0.00020465894723437541, ('import', 'duplicate')),\n",
       " (0.00019837020444419477, ('big', 'bang')),\n",
       " (0.00019807680249890274, ('san', 'diego')),\n",
       " (0.00019533256533572956, ('prime', 'minister')),\n",
       " (0.00019305019305019305, ('notre', 'dame')),\n",
       " (0.000180150217566032, ('harry', 'potter')),\n",
       " (0.0001724237025116386, ('warner', 'bros')),\n",
       " (0.00017155225591216524, ('median', 'income')),\n",
       " (0.00017005441741357234, ('guinea', 'bissau')),\n",
       " (0.00016335664813511838, ('martin', 'luther')),\n",
       " (0.00015560360785744562, ('hiv', 'aids')),\n",
       " (0.0001541867454629975, ('al', 'qaeda')),\n",
       " (0.0001486767766874814, ('middle', 'ages')),\n",
       " (0.00014614541468761417, ('providers', 'isps')),\n",
       " (0.00013783054718727233, ('des', 'moines')),\n",
       " (0.00013678630728434488, ('abraham', 'lincoln')),\n",
       " (0.00013411397929368103, ('cd', 'rom')),\n",
       " (0.00013073302777692332, ('atlantic', 'ocean')),\n",
       " (0.00013031432831425724, ('krak', 'w')),\n",
       " (0.00013014196319151473, ('fourier', 'transform')),\n",
       " (0.00012743039358445568, ('supreme', 'court')),\n",
       " (0.00012511260134120709, ('iso', 'iec')),\n",
       " (0.00012432114113721128, ('import', 'jargon')),\n",
       " (0.00012070068028957891, ('align', 'center')),\n",
       " (0.00012052782389213375, ('artificial', 'intelligence')),\n",
       " (0.00011678569824372277, ('tcp', 'ip')),\n",
       " (0.00011672243405182475, ('bertrand', 'russell')),\n",
       " (0.00011161387631975868, ('floppy', 'disk')),\n",
       " (0.00010907624337751379, ('comic', 'strip')),\n",
       " (0.00010486577181208053, ('don', 'quixote')),\n",
       " (0.00010192972666857478, ('external', 'links')),\n",
       " (0.00010136784206578305, ('broadcast', 'stations')),\n",
       " (9.936477518719614e-05, ('nautical', 'miles')),\n",
       " (9.843453510436433e-05, ('directory', 'category')),\n",
       " (9.718229009243024e-05, ('didn', 't')),\n",
       " (9.640178152836381e-05, ('jesus', 'christ')),\n",
       " (9.57339995797044e-05, ('louis', 'xiv')),\n",
       " (9.443465122135482e-05, ('alpha', 'centauri')),\n",
       " (9.437790801929084e-05, ('fatty', 'acids')),\n",
       " (9.373670561146376e-05, ('doesn', 't')),\n",
       " (9.162727811113718e-05, ('runways', 'total')),\n",
       " (9.146954978687595e-05, ('intellectual', 'property')),\n",
       " (9.095250916737185e-05, ('et', 'al')),\n",
       " (9.005715980907882e-05, ('fa', 'cup')),\n",
       " (8.874846908890822e-05, ('ibm', 'pc')),\n",
       " (8.822718500137855e-05, ('angular', 'momentum')),\n",
       " (8.497832200782957e-05, ('science', 'fiction')),\n",
       " (8.481512423295322e-05, ('armed', 'forces')),\n",
       " (8.364924426630416e-05, ('pacific', 'ocean')),\n",
       " (8.126667659926047e-05, ('ice', 'hockey')),\n",
       " (8.004340131093304e-05, ('intelligent', 'design')),\n",
       " (7.952760602023978e-05, ('committed', 'suicide')),\n",
       " (7.904403217557074e-05, ('marvel', 'comics')),\n",
       " (7.85751702462022e-05, ('paved', 'runways')),\n",
       " (7.816067184617273e-05, ('de', 'facto')),\n",
       " (7.757661896977446e-05, ('wasn', 't')),\n",
       " (7.674246043443348e-05, ('red', 'sox')),\n",
       " (7.660886044691972e-05, ('washington', 'dc')),\n",
       " (7.658712576745364e-05, ('motion', 'picture')),\n",
       " (7.656695156695156e-05, ('sexual', 'orientation')),\n",
       " (7.642961596665631e-05, ('periodic', 'table')),\n",
       " (7.510750986929007e-05, ('ottoman', 'empire')),\n",
       " (7.504042994593005e-05, ('closely', 'related')),\n",
       " (7.502019774554688e-05, ('semi', 'finalist')),\n",
       " (7.489878542510122e-05, ('summer', 'olympics')),\n",
       " (7.426673450748514e-05, ('indo', 'european')),\n",
       " (7.18203935489794e-05, ('soviet', 'union')),\n",
       " (7.158675506476293e-05, ('isn', 't')),\n",
       " (7.152071239831056e-05, ('kinetic', 'energy')),\n",
       " (7.141440190438405e-05, ('mbox', 'mbox')),\n",
       " (7.12195374614767e-05, ('vast', 'majority')),\n",
       " (7.081434583812606e-05, ('henry', 'viii')),\n",
       " (7.03358537014243e-05, ('jean', 'baptiste')),\n",
       " (6.950110840728732e-05, ('academy', 'award')),\n",
       " (6.897890778286461e-05, ('indiana', 'jones')),\n",
       " (6.895838361548805e-05, ('queen', 'elizabeth')),\n",
       " (6.890036146477675e-05, ('holy', 'spirit')),\n",
       " (6.732415146501651e-05, ('dominican', 'republic')),\n",
       " (6.724492533309488e-05, ('twentieth', 'century')),\n",
       " (6.609041449939324e-05, ('albert', 'einstein')),\n",
       " (6.600971285774906e-05, ('physicist', 'nobel')),\n",
       " (6.52709134533796e-05, ('differential', 'equations')),\n",
       " (6.510628601191445e-05, ('capita', 'income')),\n",
       " (6.466951602504107e-05, ('final', 'fantasy')),\n",
       " (6.431419628692707e-05, ('molecular', 'biology')),\n",
       " (6.411737754582723e-05, ('est', 'airports')),\n",
       " (6.39049581118215e-05, ('user', 'interface')),\n",
       " (6.377718502511695e-05, ('czech', 'republic')),\n",
       " (6.345461117044823e-05, ('don', 't')),\n",
       " (6.334860738644762e-05, ('further', 'reading')),\n",
       " (6.140047664962614e-05, ('chicago', 'illinois')),\n",
       " (6.119569757381178e-05, ('frac', 'frac')),\n",
       " (5.98986856516977e-05, ('varying', 'degrees')),\n",
       " (5.940123554569935e-05, ('dc', 'comics')),\n",
       " (5.8749193697290584e-05, ('persian', 'gulf')),\n",
       " (5.8262521960489044e-05, ('kyoto', 'protocol')),\n",
       " (5.741359254322245e-05, ('benjamin', 'franklin')),\n",
       " (5.712849340737186e-05, ('census', 'bureau')),\n",
       " (5.706785367802317e-05, ('cue', 'ball')),\n",
       " (5.706534144639445e-05, ('life', 'expectancy')),\n",
       " (5.68955431118412e-05, ('w', 'bush')),\n",
       " (5.6624339972537195e-05, ('chemist', 'nobel')),\n",
       " (5.6474293708279616e-05, ('presidential', 'election')),\n",
       " (5.572804713000557e-05, ('mickey', 'mouse')),\n",
       " (5.516195204357794e-05, ('arable', 'land')),\n",
       " (5.502042539079576e-05, ('baltic', 'sea')),\n",
       " (5.4736534812436144e-05, ('saturday', 'night')),\n",
       " (5.446267128510119e-05, ('bwv', 'anh')),\n",
       " (5.439606311817738e-05, ('roman', 'catholic')),\n",
       " (5.400953268251847e-05, ('nineteenth', 'century')),\n",
       " (5.381882216431317e-05, ('microsoft', 'windows')),\n",
       " (5.3444909263469603e-05, ('vice', 'president')),\n",
       " (5.3072055521535004e-05, ('service', 'providers')),\n",
       " (5.274591316850561e-05, ('amino', 'acid')),\n",
       " (5.270092226613966e-05, ('professional', 'wrestler')),\n",
       " (5.252038553351393e-05, ('bob', 'dylan')),\n",
       " (5.150400472900407e-05, ('heavy', 'metal')),\n",
       " (5.100060978989966e-05, ('marine', 'corps')),\n",
       " (5.077665078242437e-05, ('indian', 'ocean')),\n",
       " (5.017740139331313e-05, ('butt', 'head')),\n",
       " (4.9951659684176605e-05, ('anti', 'semitic')),\n",
       " (4.9819227374953736e-05, ('falkland', 'islands')),\n",
       " (4.9779129108377735e-05, ('foreign', 'relations')),\n",
       " (4.9589470030249574e-05, ('th', 'century')),\n",
       " (4.958401868936089e-05, ('executive', 'branch')),\n",
       " (4.773675281241079e-05, ('car', 'driver')),\n",
       " (4.742005275480869e-05, ('shortly', 'thereafter')),\n",
       " (4.702410917527578e-05, ('old', 'testament')),\n",
       " (4.6879434541105237e-05, ('web', 'site')),\n",
       " (4.685310961272193e-05, ('north', 'carolina')),\n",
       " (4.6824915963465216e-05, ('couldn', 't')),\n",
       " (4.6036347230683535e-05, ('administrative', 'divisions')),\n",
       " (4.594261792976853e-05, ('human', 'beings')),\n",
       " (4.573507018988125e-05, ('project', 'gutenberg')),\n",
       " (4.555426788824992e-05, ('southeast', 'asia')),\n",
       " (4.539192686568025e-05, ('bill', 'clinton')),\n",
       " (4.5390102887193534e-05, ('united', 'states')),\n",
       " (4.415618946328864e-05, ('purchasing', 'power')),\n",
       " (4.301246434364462e-05, ('main', 'article')),\n",
       " (4.289681146468354e-05, ('wide', 'variety')),\n",
       " (4.206209746729672e-05, ('no', 'import')),\n",
       " (4.1411618547228655e-05, ('ethnic', 'groups')),\n",
       " (4.126510542618539e-05, ('years', 'ago')),\n",
       " (4.097179166189485e-05, ('white', 'sox')),\n",
       " (4.086294076494016e-05, ('royal', 'navy')),\n",
       " (4.081569044193003e-05, ('northern', 'ireland')),\n",
       " (4.028473416228518e-05, ('baseball', 'player')),\n",
       " (4.007813752441798e-05, ('endangered', 'species')),\n",
       " (3.993965573027891e-05, ('magnetic', 'field')),\n",
       " (3.9831380489262126e-05, ('military', 'manpower')),\n",
       " (3.97886427298192e-05, ('poverty', 'line')),\n",
       " (3.968368001817239e-05, ('floating', 'point')),\n",
       " (3.9423173269774415e-05, ('privy', 'council')),\n",
       " (3.9117653354117355e-05, ('climate', 'change')),\n",
       " (3.9068859647468735e-05, ('new', 'york')),\n",
       " (3.8730309787149285e-05, ('wouldn', 't')),\n",
       " (3.806800315812154e-05, ('internet', 'explorer')),\n",
       " (3.795057728739953e-05, ('new', 'zealand')),\n",
       " (3.775437006833541e-05, ('mobile', 'cellular')),\n",
       " (3.7400654511453954e-05, ('ad', 'hoc')),\n",
       " (3.733443639350064e-05, ('communist', 'party')),\n",
       " (3.697781475559253e-05, ('league', 'baseball')),\n",
       " (3.680655979442005e-05, ('united', 'kingdom')),\n",
       " (3.669954220719202e-05, ('object', 'oriented')),\n",
       " (3.6626280088489096e-05, ('air', 'force')),\n",
       " (3.629686565889487e-05, ('faroe', 'islands')),\n",
       " (3.615614559912958e-05, ('power', 'parity')),\n",
       " (3.607516860931705e-05, ('catholic', 'church')),\n",
       " (3.5952645997143274e-05, ('byzantine', 'empire')),\n",
       " (3.578267985766445e-05, ('native', 'americans')),\n",
       " (3.554401944531279e-05, ('growth', 'rate')),\n",
       " (3.543621986655732e-05, ('isaac', 'newton')),\n",
       " (3.5378457044153025e-05, ('eastern', 'orthodox')),\n",
       " (3.522029845139057e-05, ('death', 'penalty')),\n",
       " (3.518222446686431e-05, ('rather', 'than')),\n",
       " (3.515350363252871e-05, ('net', 'migration')),\n",
       " (3.4797649712000575e-05, ('hebrew', 'bible')),\n",
       " (3.448470588235294e-05, ('wide', 'range')),\n",
       " (3.385761334605559e-05, ('christopher', 'columbus')),\n",
       " (3.3700490832372026e-05, ('human', 'rights')),\n",
       " (3.3632828775595236e-05, ('operating', 'systems')),\n",
       " (3.342717703775786e-05, ('raw', 'materials')),\n",
       " (3.34043892413052e-05, ('health', 'care')),\n",
       " (3.332331109507217e-05, ('metropolitan', 'area')),\n",
       " (3.3199538623221394e-05, ('no', 'longer')),\n",
       " (3.317485775384733e-05, ('star', 'wars')),\n",
       " (3.302044448739787e-05, ('rural', 'areas')),\n",
       " (3.294961157115045e-05, ('st', 'petersburg')),\n",
       " (3.273825606877552e-05, ('takes', 'place')),\n",
       " (3.252862378683656e-05, ('south', 'korea')),\n",
       " (3.247125441837794e-05, ('south', 'africa')),\n",
       " (3.207630710121878e-05, ('game', 'boy')),\n",
       " (3.206255300664005e-05, ('square', 'kilometers')),\n",
       " (3.205308616494987e-05, ('exchange', 'rates')),\n",
       " (3.1985670419651997e-05, ('west', 'indies')),\n",
       " (3.1964028019330495e-05, ('natural', 'selection')),\n",
       " (3.167635569433498e-05, ('hockey', 'player')),\n",
       " (3.150983303727219e-05, ('en', 'route')),\n",
       " (3.135377611378738e-05, ('holy', 'roman')),\n",
       " (3.1321071666447205e-05, ('nazi', 'germany')),\n",
       " (3.125620938088147e-05, ('trade', 'unions')),\n",
       " (3.119784110939523e-05, ('british', 'isles')),\n",
       " (3.1018696786890854e-05, ('foreign', 'affairs')),\n",
       " (3.09825422706396e-05, ('foreign', 'policy')),\n",
       " (3.094308890644906e-05, ('official', 'website')),\n",
       " (3.093573564397592e-05, ('international', 'airport')),\n",
       " (3.0742217335927004e-05, ('roman', 'empire')),\n",
       " (3.0386326153231047e-05, ('nuclear', 'weapons')),\n",
       " (3.0062367976781395e-05, ('world', 'factbook')),\n",
       " (3.003837613100101e-05, ('nobel', 'peace')),\n",
       " (2.984516626518044e-05, ('north', 'america')),\n",
       " (2.9706189868173477e-05, ('na', 'na')),\n",
       " (2.9674479131218368e-05, ('political', 'parties')),\n",
       " (2.944423997055576e-05, ('prime', 'ministers')),\n",
       " (2.9377790459334246e-05, ('operating', 'system')),\n",
       " (2.9332275486610964e-05, ('days', 'remaining')),\n",
       " (2.9187570605728327e-05, ('olympic', 'games')),\n",
       " (2.900954172176465e-05, ('bits', 'added')),\n",
       " (2.897522530432767e-05, ('signal', 'processing')),\n",
       " (2.873232961728537e-05, ('eldest', 'son')),\n",
       " (2.858112478479014e-05, ('natural', 'gas')),\n",
       " (2.8293650365931213e-05, ('catholic', 'encyclopedia')),\n",
       " (2.7959956479719915e-05, ('general', 'relativity')),\n",
       " (2.7934598572770333e-05, ('civil', 'war')),\n",
       " (2.791021031612119e-05, ('mental', 'illness')),\n",
       " (2.7814864263462394e-05, ('bronze', 'age')),\n",
       " (2.7803521779425393e-05, ('babe', 'ruth')),\n",
       " (2.7484427323478516e-05, ('mortality', 'rate')),\n",
       " (2.748061635096673e-05, ('diplomatic', 'relations')),\n",
       " (2.7398274507645043e-05, ('carried', 'out')),\n",
       " (2.730974411670084e-05, ('once', 'again')),\n",
       " (2.7193015788573708e-05, ('her', 'husband')),\n",
       " (2.7176831456494805e-05, ('national', 'anthem')),\n",
       " (2.7164723712214024e-05, ('e', 'g')),\n",
       " (2.6942488454733404e-05, ('role', 'playing')),\n",
       " (2.691312147006904e-05, ('right', 'wing')),\n",
       " (2.6824969188248973e-05, ('leap', 'years')),\n",
       " (2.679394006460142e-05, ('university', 'press')),\n",
       " (2.6751683516883323e-05, ('peace', 'prize')),\n",
       " (2.6732038794812575e-05, ('de', 'la')),\n",
       " (2.6666666666666667e-05, ('margaret', 'thatcher')),\n",
       " (2.6376072681905883e-05, ('new', 'orleans')),\n",
       " (2.6308153593190832e-05, ('austria', 'hungary')),\n",
       " (2.6242283182910884e-05, ('both', 'sides')),\n",
       " (2.603492160736333e-05, ('natural', 'resources')),\n",
       " (2.5966538898589946e-05, ('births', 'living')),\n",
       " (2.592736633516677e-05, ('singer', 'songwriter')),\n",
       " (2.57912441070915e-05, ('head', 'coach')),\n",
       " (2.573801652718209e-05, ('box', 'office')),\n",
       " (2.5720164609053497e-05, ('addison', 'wesley')),\n",
       " (2.5422403003754694e-05, ('latin', 'alphabet')),\n",
       " (2.5404752140350367e-05, ('football', 'league')),\n",
       " (2.5197460871250667e-05, ('space', 'shuttle')),\n",
       " (2.5128808453909267e-05, ('took', 'place')),\n",
       " (2.5123542778439514e-05, ('comic', 'book')),\n",
       " (2.510338410353305e-05, ('mathbf', 'mathbf')),\n",
       " (2.5083102708138988e-05, ('open', 'source')),\n",
       " (2.497888881010243e-05, ('east', 'timor')),\n",
       " (2.496010631518305e-05, ('e', 'mail')),\n",
       " (2.4920696804834395e-05, ('left', 'wing')),\n",
       " (2.49011615434123e-05, ('lesser', 'extent')),\n",
       " (2.486216674939601e-05, ('est', 'gdp')),\n",
       " (2.475949955337672e-05, ('day', 'saints')),\n",
       " (2.4686338732530225e-05, ('middle', 'east')),\n",
       " (2.456315375778463e-05, ('great', 'britain')),\n",
       " (2.435191097518967e-05, ('european', 'union')),\n",
       " (2.4341244329942247e-05, ('security', 'council')),\n",
       " (2.4210483394156863e-05, ('m', 'ori')),\n",
       " (2.4150042232881232e-05, ('pointed', 'out')),\n",
       " (2.4112212449810428e-05, ('industrial', 'revolution')),\n",
       " (2.411045804512406e-05, ('basketball', 'player')),\n",
       " (2.3953388986307787e-05, ('roman', 'catholics')),\n",
       " (2.3941200411788646e-05, ('heavily', 'influenced')),\n",
       " (2.390394438986377e-05, ('windows', 'xp')),\n",
       " (2.3843945143866835e-05, ('led', 'zeppelin')),\n",
       " (2.380413953986598e-05, ('james', 'bond')),\n",
       " (2.3703703703703703e-05, ('programming', 'language')),\n",
       " (2.346581493470224e-05, ('joseph', 'smith')),\n",
       " (2.3440015454955244e-05, ('remaining', 'events')),\n",
       " (2.3410742490446948e-05, ('st', 'louis')),\n",
       " (2.334983433024154e-05, ('short', 'lived')),\n",
       " (2.3343668435379567e-05, ('civil', 'rights')),\n",
       " (2.3169601482854495e-05, ('dia', 'britannica')),\n",
       " (2.310034327110101e-05, ('os', 'x')),\n",
       " (2.304338632047457e-05, ('divided', 'into')),\n",
       " (2.279898914981857e-05, ('machine', 'guns')),\n",
       " (2.2543380775979724e-05, ('th', 'anniversary')),\n",
       " (2.243516854000324e-05, ('programming', 'languages')),\n",
       " (2.2185110581881778e-05, ('greek', 'mythology')),\n",
       " (2.206287920573635e-05, ('great', 'deal')),\n",
       " (2.2006039085869136e-05, ('cayman', 'islands')),\n",
       " (2.188571599386773e-05, ('almost', 'exclusively')),\n",
       " (2.1816870348710816e-05, ('labour', 'party')),\n",
       " (2.1779137763935926e-05, ('pulitzer', 'prize')),\n",
       " (2.17173094901994e-05, ('kansas', 'city')),\n",
       " (2.1713826867161672e-05, ('orthodox', 'church')),\n",
       " (2.1654964236498456e-05, ('carbon', 'atoms')),\n",
       " (2.1631419263524767e-05, ('oxford', 'university')),\n",
       " (2.1247000907851212e-05, ('i', 'am')),\n",
       " (2.1201209656218146e-05, ('short', 'stories')),\n",
       " (2.115701702221597e-05, ('distinction', 'between')),\n",
       " (2.1055098446316824e-05, ('machine', 'gun')),\n",
       " (2.1046373052223944e-05, ('karl', 'marx')),\n",
       " (2.0989313281669027e-05, ('war', 'ii')),\n",
       " (2.090648553615543e-05, ('south', 'carolina')),\n",
       " (2.0903756048068443e-05, ('special', 'relativity')),\n",
       " (2.090093102836905e-05, ('d', 'ivoire')),\n",
       " (2.0717052041439847e-05, ('x', 'y')),\n",
       " (2.0669456743769733e-05, ('democratic', 'republic')),\n",
       " (2.0654259021384232e-05, ('picked', 'up')),\n",
       " (2.0347663305650078e-05, ('charles', 'darwin')),\n",
       " (2.0319084888621312e-05, ('federal', 'reserve')),\n",
       " (2.019473494410386e-05, ('judicial', 'branch')),\n",
       " (2.015203772031551e-05, ('open', 'quarter')),\n",
       " (2.014496865750173e-05, ('race', 'car')),\n",
       " (1.9981620822013064e-05, ('united', 'nations')),\n",
       " (1.996937566007469e-05, ('f', 'kennedy')),\n",
       " (1.9933257490775098e-05, ('american', 'actress')),\n",
       " (1.9884131190725406e-05, ('domestic', 'violence')),\n",
       " (1.9751135690302193e-05, ('asia', 'minor')),\n",
       " (1.9739316940676586e-05, ('red', 'cross')),\n",
       " (1.9699664815285392e-05, ('shortly', 'after')),\n",
       " (1.959775605693148e-05, ('attorney', 'general')),\n",
       " (1.9584708259790756e-05, ('mississippi', 'river')),\n",
       " (1.9557133776010264e-05, ('large', 'scale')),\n",
       " (1.944287605244288e-05, ('stock', 'exchange')),\n",
       " (1.9347870951584815e-05, ('football', 'player')),\n",
       " (1.9067473432653685e-05, ('immune', 'system')),\n",
       " (1.9029185333391327e-05, ('nervous', 'system')),\n",
       " (1.901101567094451e-05, ('democratic', 'party')),\n",
       " (1.8947827157920666e-05, ('electricity', 'production')),\n",
       " (1.8749062546872656e-05, ('legislative', 'branch')),\n",
       " (1.8680415452439663e-05, ('law', 'enforcement')),\n",
       " (1.8614226767441005e-05, ('video', 'game')),\n",
       " (1.8545011391935568e-05, ('morse', 'code')),\n",
       " (1.8531890680330663e-05, ('difference', 'between')),\n",
       " (1.8447503193812005e-05, ('ph', 'd')),\n",
       " (1.8391450316332946e-05, ('mediterranean', 'sea')),\n",
       " (1.826630815992518e-05, ('tennis', 'player')),\n",
       " (1.8230262235310615e-05, ('new', 'jersey')),\n",
       " (1.8218831935236013e-05, ('central', 'asia')),\n",
       " (1.8127496926497754e-05, ('american', 'actor')),\n",
       " (1.7986668281469774e-05, ('harvard', 'university')),\n",
       " (1.7764243116355794e-05, ('great', 'lakes')),\n",
       " (1.7677875119410646e-05, ('chief', 'justice')),\n",
       " (1.7601498690166872e-05, ('south', 'wales')),\n",
       " (1.7579282564365287e-05, ('bbc', 'news')),\n",
       " (1.755226760034555e-05, ('national', 'assembly')),\n",
       " (1.7281144277614937e-05, ('african', 'americans')),\n",
       " (1.713195466975179e-05, ('new', 'testament')),\n",
       " (1.7122603548944963e-05, ('cape', 'breton')),\n",
       " (1.6945735160562276e-05, ('cold', 'war')),\n",
       " (1.6775196194906212e-05, ('best', 'selling')),\n",
       " (1.6739927552950863e-05, ('free', 'software')),\n",
       " (1.672558634583951e-05, ('counter', 'strike')),\n",
       " (1.6710943827071166e-05, ('roman', 'emperor')),\n",
       " (1.668704037875701e-05, ('british', 'columbia')),\n",
       " (1.6223335903366744e-05, ('total', 'population')),\n",
       " (1.6153293340981425e-05, ('th', 'centuries')),\n",
       " (1.60567684458602e-05, ('radio', 'stations')),\n",
       " (1.605205039831735e-05, ('high', 'school')),\n",
       " (1.6015203766775925e-05, ('dark', 'matter')),\n",
       " (1.5965833437365312e-05, ('present', 'day')),\n",
       " (1.5869608838350012e-05, ('st', 'earl')),\n",
       " (1.5852918733711124e-05, ('private', 'sector')),\n",
       " (1.5788808892257167e-05, ('episcopal', 'church')),\n",
       " (1.5707820888116604e-05, ('video', 'games')),\n",
       " (1.5605112935635436e-05, ('george', 'washington')),\n",
       " (1.5575442314070057e-05, ('cambridge', 'university')),\n",
       " (1.554179040198791e-05, ('does', 'not')),\n",
       " (1.544146589485541e-05, ('g', 'del')),\n",
       " (1.543108876749925e-05, ('american', 'musician')),\n",
       " (1.5426955625958033e-05, ('vietnam', 'war')),\n",
       " (1.54214594143474e-05, ('fan', 'fiction')),\n",
       " (1.5412237634459857e-05, ('great', 'depression')),\n",
       " (1.5403285520801587e-05, ('bah', 'faith')),\n",
       " (1.5356445937355385e-05, ('folk', 'music')),\n",
       " (1.524687743950039e-05, ('web', 'browser')),\n",
       " (1.5206645642071044e-05, ('air', 'pollution')),\n",
       " (1.5130197942108987e-05, ('at', 'least')),\n",
       " (1.507682192361126e-05, ('black', 'hole')),\n",
       " (1.4982930984009754e-05, ('tv', 'series')),\n",
       " (1.497349164742784e-05, ('frac', 'right')),\n",
       " (1.4913858129056498e-05, ('princeton', 'university')),\n",
       " (1.485736305890272e-05, ('latter', 'day')),\n",
       " (1.4804970142361176e-05, ('crown', 'prince')),\n",
       " (1.4739437917204043e-05, ('film', 'director')),\n",
       " (1.467768991133477e-05, ('golden', 'age')),\n",
       " (1.4672088510896518e-05, ('y', 'z')),\n",
       " (1.4648795136600015e-05, ('major', 'league')),\n",
       " (1.4629002964443242e-05, ('vector', 'space')),\n",
       " (1.4551191909423728e-05, ('governor', 'general')),\n",
       " (1.4405235648374745e-05, ('new', 'brunswick')),\n",
       " (1.4272864860206713e-05, ('non', 'profit')),\n",
       " (1.4231800373682135e-05, ('public', 'domain')),\n",
       " (1.4139525872793482e-05, ('real', 'numbers')),\n",
       " (1.3866163787126654e-05, ('classical', 'mechanics')),\n",
       " (1.3819317697571016e-05, ('solar', 'system')),\n",
       " (1.3744894074411917e-05, ('full', 'text')),\n",
       " (1.3700265589434355e-05, ('short', 'story')),\n",
       " (1.3504106695303947e-05, ('most', 'notably')),\n",
       " (1.3474051426779554e-05, ('un', 'security')),\n",
       " (1.346911434404128e-05, ('economic', 'growth')),\n",
       " (1.3424422768728583e-05, ('sea', 'level')),\n",
       " (1.3374603538537965e-05, ('minor', 'bwv')),\n",
       " (1.3324450366422385e-05, ('fertility', 'rate')),\n",
       " (1.331303096111763e-05, ('television', 'series')),\n",
       " (1.3270111501302737e-05, ('radio', 'broadcast')),\n",
       " (1.3238483432278825e-05, ('york', 'city')),\n",
       " (1.3183568000843748e-05, ('foreign', 'investment')),\n",
       " (1.3157833452103158e-05, ('official', 'site')),\n",
       " (1.3118798893154059e-05, ('did', 'not')),\n",
       " (1.3116595187131273e-05, ('broke', 'out')),\n",
       " (1.3089957966416108e-05, ('relationship', 'between')),\n",
       " (1.3020287053928583e-05, ('latin', 'america')),\n",
       " (1.2881131058220509e-05, ('west', 'coast')),\n",
       " (1.285069883842751e-05, ('australian', 'open')),\n",
       " (1.2843793466726963e-05, ('we', 'know')),\n",
       " (1.2843565373747753e-05, ('north', 'korea')),\n",
       " (1.2800107440163944e-05, ('world', 'war')),\n",
       " (1.2772472063520587e-05, ('decision', 'making')),\n",
       " (1.2688192406022156e-05, ('c', 'te')),\n",
       " (1.2687254478130933e-05, ('military', 'expenditures')),\n",
       " (1.2668710760173945e-05, ('recent', 'years')),\n",
       " (1.2596917534279362e-05, ('lung', 'cancer')),\n",
       " (1.2591373155825296e-05, ('community', 'college')),\n",
       " (1.2559492332099418e-05, ('mentioned', 'above')),\n",
       " (1.2418812016442507e-05, ('crude', 'oil')),\n",
       " (1.238225667373798e-05, ('new', 'hampshire')),\n",
       " (1.2254384562401871e-05, ('not', 'necessarily')),\n",
       " (1.2241323957470067e-05, ('even', 'though')),\n",
       " (1.22396707915073e-05, ('rock', 'band')),\n",
       " (1.2227848844223727e-05, ('feast', 'day')),\n",
       " (1.2173881410067742e-05, ('cia', 'world')),\n",
       " (1.212538808194493e-05, ('american', 'singer')),\n",
       " (1.2108967965406591e-05, ('aircraft', 'carrier')),\n",
       " (1.209141314146233e-05, ('east', 'asia')),\n",
       " (1.2090306184919598e-05, ('less', 'than'))]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta = 100  # You can play with this hyperparameter to see how it affects the results.\n",
    "unigrams, bigrams = word_utils.unigram_and_bigram_counts(words)\n",
    "scored_bigrams = sorted(\n",
    "    [(word_stream.score_bigram(bigram, unigrams, bigrams, delta), bigram) for bigram in bigrams],\n",
    "    reverse=True)\n",
    "scored_bigrams[0:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next cell uses the scores computed above and calls grouped_stream which takes a list of words and a set of n-grams and returns the list of words with those n-grams combined into single tokens.\n",
    "\n",
    "e.g. ['the', 'supreme', 'court'] => ['the', 'supreme_court']\n",
    "\n",
    "(You can find more examples in the tests for the function in word_stream_test.py.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# You can leave this cell alone.\n",
    "\n",
    "# If you want to come back afterwards, you can experiment with different phrase_thresholds.\n",
    "# You should set phrase_threshold to a value at which the bigrams in the previous\n",
    "# output start looking less tightly coupled.  grouped_stream below will re-tokenize\n",
    "# a stream of words to consider bigrams scoring above phrase_threshold as a single token.\n",
    "\n",
    "phrase_threshold = 1.0\n",
    "phrases = [bigram for score, bigram in scored_bigrams if score >= phrase_threshold]\n",
    "words = word_utils.grouped_stream(words, phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Cleanup some variables to recover some memory.\n",
    "del unigrams\n",
    "del bigrams\n",
    "del scored_bigrams\n",
    "del phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. TensorFlow for GloVe\n",
    "\n",
    "### Cooccurrence Table\n",
    "In this section, we first build the cooccurrence table with context window of size C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 20000  # Amount of vocabulary to keep.  Lower frequency words are mapped to <UNK> (word id: 0).\n",
    "\n",
    "# Map each of the words to a wordid.  Only the most popular VOCAB_SIZE words are kept.\n",
    "vocabulary = word_utils.Vocabulary(words, VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert the word stream to wordids.\n",
    "# We need to do this because the TensorFlow code you will write in the\n",
    "# next section will use an API that expects indexes into the embedding\n",
    "# matrix, not words.\n",
    "wordids = [vocabulary.to_id(word) for word in words]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Building the cooccurrence table takes a considerable amount of time\n",
    "# with the Wikipedia set.\n",
    "\n",
    "C = 10  # Context window size.\n",
    "ctable = word_stream.cooccurrence_table(wordids, C)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<UNK>', '<UNK>', 706138.1285682213),\n",
       " ('the', 'of', 374048.6742070548),\n",
       " ('of', 'the', 374048.57420705474),\n",
       " ('<UNK>', 'the', 342798.24642992363),\n",
       " ('the', '<UNK>', 342798.24642992363),\n",
       " ('the', 'the', 269496.53413024516),\n",
       " ('zero', 'zero', 264773.4888895979),\n",
       " ('one', 'nine', 257475.82539757458),\n",
       " ('nine', 'one', 257475.71428646345),\n",
       " ('<UNK>', 'and', 193557.81507994025)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Output top words occurring within the same context.\n",
    "# If everything has worked properly, you should see a considerable number of \"<UNK>\", \"the\", \"of\", and numbers.\n",
    "sorted([(vocabulary.to_word(word), vocabulary.to_word(context_word), count) for word, context_word, count in ctable if count > len(words) / 100], key=lambda x: x[2], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Shard the table into word lists, context word lists and corresponding counts.\n",
    "# We are going to provide these to TensorFlow as their own entry in the feed_dict,\n",
    "# so we do this separation once, up front.\n",
    "ctable_wids = np.array([word for word, _, _ in ctable])\n",
    "ctable_cwids = np.array([context_word for _, context_word, _ in ctable])\n",
    "ctable_counts = np.array([count for _, _, count in ctable])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0, 706138.12856822135),\n",
       " (0, 1, 107923.21904752152),\n",
       " (0, 2, 117.23730158730162),\n",
       " (0, 3, 29.944841269841259),\n",
       " (0, 4, 50.867857142857197)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here is what our final data looks like.  It's similar to the table two\n",
    "# cells previous, except instead of words, there are wordids.\n",
    "zip(ctable_wids[:5], ctable_cwids[:5], ctable_counts[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow Graph Setup (12 points)\n",
    "\n",
    "Complete the functions in **glove.py** using the TensorFlow API and then run the corresponding tests in the cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_embedding_lookup (glove_test.TestGlove) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.170s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=1 errors=0 failures=0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(glove)\n",
    "reload(glove_test)\n",
    "unittest.TextTestRunner(verbosity=2).run(\n",
    "    unittest.TestLoader().loadTestsFromName(\n",
    "        'TestGlove.test_embedding_lookup', glove_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_example_weight (glove_test.TestGlove) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.044s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=1 errors=0 failures=0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(glove)\n",
    "reload(glove_test)\n",
    "unittest.TextTestRunner(verbosity=2).run(\n",
    "    unittest.TestLoader().loadTestsFromName(\n",
    "        'TestGlove.test_example_weight', glove_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_loss (glove_test.TestGlove) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.095s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=1 errors=0 failures=0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(glove)\n",
    "reload(glove_test)\n",
    "unittest.TextTestRunner(verbosity=2).run(\n",
    "    unittest.TestLoader().loadTestsFromName(\n",
    "        'TestGlove.test_loss', glove_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Hyperparameters.\n",
    "\n",
    "# You may want to shrink num_examples_to_train to finish debugging\n",
    "# and only run it this long once you are training on Wikipedia.\n",
    "learning_rate = 0.003\n",
    "num_examples_to_train = 300000000\n",
    "#num_examples_to_train = 3000\n",
    "batch_size = 100\n",
    "embedding_dim = 300\n",
    "\n",
    "# Construct the training graph.\n",
    "tf.reset_default_graph()\n",
    "\n",
    "wids_ph = tf.placeholder(tf.int32, shape=[None])\n",
    "c_wids_ph = tf.placeholder(tf.int32, shape=[None])\n",
    "counts_ph = tf.placeholder(tf.float32, shape=[None])\n",
    "\n",
    "with tf.variable_scope('word_embeddings'):\n",
    "    word_embeddings, word_bias, word_embed_matrix = (\n",
    "        glove.wordids_to_tensors(wids_ph, embedding_dim, vocabulary.size()))\n",
    "with tf.variable_scope('word_context_embeddings'):\n",
    "    word_context_embeddings, word_context_bias, word_context_embed_matrix = (\n",
    "        glove.wordids_to_tensors(c_wids_ph, embedding_dim, vocabulary.size()))\n",
    "    \n",
    "losses = glove.loss(\n",
    "    word_embeddings, word_bias, word_context_embeddings, word_context_bias,\n",
    "    tf.cast(counts_ph, tf.float32))\n",
    "loss = tf.reduce_mean(losses)\n",
    "\n",
    "# Adam is similar to AdaGrad in that it handles sparse gradients well.\n",
    "# Specifically, you may imagine that some words appear with more context\n",
    "# words than others and with bigger counts.  They therefore are updated\n",
    "# more often and more aggressively (remember the weighting function\n",
    "# you implemented).  Adam backs off updating parameters that it has already\n",
    "# significantly moved around.  (intuitively: the 500th time you backprop\n",
    "# into \"the\", you probably don't have a lot more information to add).\n",
    "#\n",
    "# Here is the original University of Toronto paper detailing the word\n",
    "# done in collaboration with Google DeepMind.\n",
    "# https://arxiv.org/pdf/1412.6980v8.pdf\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train the embeddings.\n",
    "# Set up the session & initialize variables.\n",
    "sess = tf.Session()\n",
    "sess.run(tf.initialize_all_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training examples: 31302717\n",
      "# of epochs: 9.58383261108\n",
      "# batches: 3000001\n",
      "Initial loss: 1.61115\n",
      "0 : 1.87783\n",
      "Expected time left: 48.6420211555 hours ( 58.3898692131 seconds per 1000 batches).\n",
      "1000 : 1.63667\n",
      "Expected time left: 54.3404595704 hours ( 65.2520310879 seconds per 1000 batches).\n",
      "2000 : 2.07162\n",
      "Expected time left: 55.5742327687 hours ( 66.7558128834 seconds per 1000 batches).\n",
      "3000 : 1.08939\n",
      "Expected time left: 56.2798013419 hours ( 67.6259069443 seconds per 1000 batches).\n",
      "4000 : 1.19348\n",
      "Expected time left: 55.9276270815 hours ( 67.2251720428 seconds per 1000 batches).\n",
      "5000 : 1.45329\n",
      "Expected time left: 56.7409197772 hours ( 68.2255320549 seconds per 1000 batches).\n",
      "6000 : 1.58094\n",
      "Expected time left: 56.1176747814 hours ( 67.4986841679 seconds per 1000 batches).\n",
      "7000 : 0.880944\n",
      "Expected time left: 56.2169902893 hours ( 67.6407411098 seconds per 1000 batches).\n",
      "8000 : 2.48954\n",
      "Expected time left: 56.9559065423 hours ( 68.5527231693 seconds per 1000 batches).\n",
      "9000 : 2.23745\n",
      "Expected time left: 56.4584497223 hours ( 67.976706028 seconds per 1000 batches).\n",
      "10000 : 1.87337\n",
      "Expected time left: 55.5596941228 hours ( 66.9169728756 seconds per 1000 batches).\n",
      "11000 : 0.782413\n",
      "Expected time left: 55.0329185416 hours ( 66.3046989441 seconds per 1000 batches).\n",
      "12000 : 1.7516\n",
      "Expected time left: 55.7633408804 hours ( 67.2072179317 seconds per 1000 batches).\n",
      "13000 : 1.13396\n",
      "Expected time left: 56.0370116929 hours ( 67.5596699715 seconds per 1000 batches).\n",
      "14000 : 0.96672\n",
      "Expected time left: 56.1443978564 hours ( 67.7118139267 seconds per 1000 batches).\n",
      "15000 : 1.2595\n",
      "Expected time left: 56.4474831947 hours ( 68.1001579762 seconds per 1000 batches).\n",
      "16000 : 1.22917\n",
      "Expected time left: 55.6091444289 hours ( 67.1112480164 seconds per 1000 batches).\n",
      "17000 : 0.572331\n",
      "Expected time left: 55.2341640752 hours ( 66.6810610294 seconds per 1000 batches).\n",
      "18000 : 0.58469\n",
      "Expected time left: 55.5512268412 hours ( 67.086329937 seconds per 1000 batches).\n",
      "19000 : 0.502818\n",
      "Expected time left: 55.6978975454 hours ( 67.2860281467 seconds per 1000 batches).\n",
      "20000 : 0.451211\n",
      "Expected time left: 55.5670955291 hours ( 67.1505460739 seconds per 1000 batches).\n",
      "21000 : 1.22115\n",
      "Expected time left: 55.9222885133 hours ( 67.6024751663 seconds per 1000 batches).\n",
      "22000 : 0.913468\n",
      "Expected time left: 56.1323544542 hours ( 67.8792099953 seconds per 1000 batches).\n",
      "23000 : 0.528795\n",
      "Expected time left: 56.3838027698 hours ( 68.2061901093 seconds per 1000 batches).\n",
      "24000 : 0.44068\n",
      "Expected time left: 56.0053415069 hours ( 67.7711467743 seconds per 1000 batches).\n",
      "25000 : 0.639222\n",
      "Expected time left: 57.0249523601 hours ( 69.0281639099 seconds per 1000 batches).\n",
      "26000 : 0.349045\n",
      "Expected time left: 56.2146515654 hours ( 68.0701909065 seconds per 1000 batches).\n",
      "27000 : 0.58773\n",
      "Expected time left: 55.9692899834 hours ( 67.7958869934 seconds per 1000 batches).\n",
      "28000 : 0.556805\n",
      "Expected time left: 56.6526752273 hours ( 68.6467728615 seconds per 1000 batches).\n",
      "29000 : 0.4566\n",
      "Expected time left: 56.6183765549 hours ( 68.6283121109 seconds per 1000 batches).\n",
      "30000 : 0.331029\n",
      "Expected time left: 56.7888937358 hours ( 68.8581840992 seconds per 1000 batches).\n",
      "31000 : 0.390155\n",
      "Expected time left: 56.4882399789 hours ( 68.516710043 seconds per 1000 batches).\n",
      "32000 : 0.33235\n",
      "Expected time left: 56.440848761 hours ( 68.4823009968 seconds per 1000 batches).\n",
      "33000 : 0.339859\n",
      "Expected time left: 56.1807501054 hours ( 68.1896939278 seconds per 1000 batches).\n",
      "34000 : 0.491384\n",
      "Expected time left: 56.6490809295 hours ( 68.7813229561 seconds per 1000 batches).\n",
      "35000 : 0.569843\n",
      "Expected time left: 56.4624578013 hours ( 68.5778608322 seconds per 1000 batches).\n",
      "36000 : 0.311011\n",
      "Expected time left: 56.2940469516 hours ( 68.3963890076 seconds per 1000 batches).\n",
      "37000 : 0.472477\n",
      "Expected time left: 56.0486038309 hours ( 68.1211700439 seconds per 1000 batches).\n",
      "38000 : 0.638039\n",
      "Expected time left: 55.7269221283 hours ( 67.7530739307 seconds per 1000 batches).\n",
      "39000 : 0.303402\n",
      "Expected time left: 55.4726362557 hours ( 67.4666969776 seconds per 1000 batches).\n",
      "40000 : 0.230571\n",
      "Expected time left: 55.4768393103 hours ( 67.4946110249 seconds per 1000 batches).\n",
      "41000 : 0.402559\n",
      "Expected time left: 55.8967057364 hours ( 68.0284221172 seconds per 1000 batches).\n",
      "42000 : 0.529906\n",
      "Expected time left: 55.7665493374 hours ( 67.8929691315 seconds per 1000 batches).\n",
      "43000 : 0.403941\n",
      "Expected time left: 56.3492034722 hours ( 68.6255290508 seconds per 1000 batches).\n",
      "44000 : 0.296542\n",
      "Expected time left: 58.1834460432 hours ( 70.8833620548 seconds per 1000 batches).\n",
      "45000 : 0.277885\n",
      "Expected time left: 55.5301798646 hours ( 67.6738591194 seconds per 1000 batches).\n",
      "46000 : 0.255543\n",
      "Expected time left: 55.8057103861 hours ( 68.0326750278 seconds per 1000 batches).\n",
      "47000 : 0.251805\n",
      "Expected time left: 55.7034298719 hours ( 67.930989027 seconds per 1000 batches).\n",
      "48000 : 0.236633\n",
      "Expected time left: 55.5399980854 hours ( 67.7546341419 seconds per 1000 batches).\n",
      "49000 : 0.260978\n",
      "Expected time left: 56.2135704608 hours ( 68.5995881557 seconds per 1000 batches).\n",
      "50000 : 0.194613\n",
      "Expected time left: 57.9848043002 hours ( 70.7850880623 seconds per 1000 batches).\n",
      "51000 : 0.321812\n",
      "Expected time left: 56.0629112839 hours ( 68.4621479511 seconds per 1000 batches).\n",
      "52000 : 0.318389\n",
      "Expected time left: 56.5703299473 hours ( 69.1052320004 seconds per 1000 batches).\n",
      "53000 : 0.244356\n",
      "Expected time left: 56.8585443293 hours ( 69.4808859825 seconds per 1000 batches).\n",
      "54000 : 0.187964\n",
      "Expected time left: 55.4735930063 hours ( 67.8114998341 seconds per 1000 batches).\n",
      "55000 : 0.287181\n",
      "Expected time left: 55.0841820797 hours ( 67.3583519459 seconds per 1000 batches).\n",
      "56000 : 0.321282\n",
      "Expected time left: 55.4555139772 hours ( 67.8354680538 seconds per 1000 batches).\n",
      "57000 : 0.217588\n",
      "Expected time left: 55.0599941808 hours ( 67.3745450974 seconds per 1000 batches).\n",
      "58000 : 0.327873\n",
      "Expected time left: 54.9492124874 hours ( 67.2618489265 seconds per 1000 batches).\n",
      "59000 : 0.240912\n",
      "Expected time left: 55.4422646314 hours ( 67.8884642124 seconds per 1000 batches).\n",
      "60000 : 0.260707\n",
      "Expected time left: 55.6505015933 hours ( 68.1666340828 seconds per 1000 batches).\n",
      "61000 : 0.17787\n",
      "Expected time left: 55.5622334983 hours ( 68.0816788673 seconds per 1000 batches).\n",
      "62000 : 0.27982\n",
      "Expected time left: 55.6979216884 hours ( 68.2711780071 seconds per 1000 batches).\n",
      "63000 : 0.19068\n",
      "Expected time left: 55.5500148794 hours ( 68.1130740643 seconds per 1000 batches).\n",
      "64000 : 0.18695\n",
      "Expected time left: 55.19881413 hours ( 67.70550704 seconds per 1000 batches).\n",
      "65000 : 0.202579\n",
      "Expected time left: 54.6890605404 hours ( 67.1031188965 seconds per 1000 batches).\n",
      "66000 : 0.203081\n",
      "Expected time left: 54.8682004419 hours ( 67.3458759785 seconds per 1000 batches).\n",
      "67000 : 0.19344\n"
     ]
    }
   ],
   "source": [
    "# Important note:  You do not need to run this cell to completion.\n",
    "# Let it train for 30 minutes or so, then interrupt the kernel and see how good\n",
    "# the nearest-neighbors results look.  Run this cell again to pick up from where\n",
    "# you left off.\n",
    "\n",
    "# An hour on the recommended GCE cloud instance gets reasonably good results.\n",
    "# Two hours cleans up the vectors beautifully.\n",
    "\n",
    "REPORT_LOSS_EVERY = 1000\n",
    "EVAL_BATCH_SIZE = 5000\n",
    "\n",
    "indexes = range(len(ctable_wids))\n",
    "\n",
    "def make_feed_dict(feed_dict_batch_size):\n",
    "    batch_idx = random.sample(indexes, feed_dict_batch_size)\n",
    "    batch_wids = ctable_wids[batch_idx]\n",
    "    batch_cwids = ctable_cwids[batch_idx]\n",
    "    batch_counts = ctable_counts[batch_idx]\n",
    "    return {\n",
    "        wids_ph: batch_wids,\n",
    "        c_wids_ph: batch_cwids,\n",
    "        counts_ph: batch_counts\n",
    "    }\n",
    "\n",
    "num_batches = num_examples_to_train / batch_size + 1\n",
    "\n",
    "print '# training examples:', len(ctable_wids)\n",
    "print '# of epochs:', 1.0 * num_examples_to_train / len(ctable_wids)\n",
    "print '# batches:', num_batches\n",
    "print 'Initial loss:', sess.run(loss, feed_dict=make_feed_dict(EVAL_BATCH_SIZE))\n",
    "\n",
    "current_timer = None\n",
    "for batch in xrange(num_batches):\n",
    "    # Train based on randomly sampled batches of examples.\n",
    "    loss_val, _ = sess.run([loss, train_op], feed_dict=make_feed_dict(batch_size))\n",
    "    \n",
    "    # Do some basic reporting as training progresses.\n",
    "    if batch % REPORT_LOSS_EVERY == 0:\n",
    "        if current_timer:\n",
    "            remaining_reporting_cycles = 1.0 * (num_batches - batch) / REPORT_LOSS_EVERY\n",
    "            cycle_time = time.time() - current_timer\n",
    "            print 'Expected time left:', remaining_reporting_cycles * cycle_time / 60 / 60, 'hours (', cycle_time, 'seconds per', REPORT_LOSS_EVERY, 'batches).'\n",
    "        current_timer = time.time()\n",
    "            \n",
    "        print batch, ':', sess.run(loss, feed_dict=make_feed_dict(EVAL_BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play! (1 point)\n",
    "\n",
    "Congratulations!  You now have some embeddings.  We only trained a short while over not a lot of text, but these still work reasonably well.\n",
    "\n",
    "If you want more compelling vectors, scroll back up to the top of this notebook and follow the instructions to switch the data source to Wikipedia and execute it again.  Note:  training these vectors for a long time is **not required**, but since you've gotten this far, it takes almost no additional effort to see the result of your hard work below.  The longer you run on the Wikipedia set, the nicer your word vectors will be.  As noted in the previous cell, you can let it run for a while, interrupt the kernel, see how things look, and then run that cell again to have it pick up from where it left off.\n",
    "\n",
    "We have a number of suggestions below to get you started exploring the space.  Feel free to try some of your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_nn_cos(v, Wv, k=10):\n",
    "    \"\"\"Find nearest neighbors, by cosine distance.\"\"\"\n",
    "    Z = np.linalg.norm(Wv, axis=1) * np.linalg.norm(v)\n",
    "    ds = np.dot(Wv, v.T) / Z\n",
    "    nns = np.argsort(-1*ds)[:k]  # sort descending, take best\n",
    "    return nns, ds[nns]  # word indices, distances\n",
    "\n",
    "def show_nns(v, Wv, vocabulary, k=10):\n",
    "    print \"Nearest neighbors:\"\n",
    "    for i, d in zip(*find_nn_cos(v, Wv, k)):\n",
    "        w = vocabulary.to_word(i)\n",
    "        print \"%.03f : \\\"%s\\\"\" % (d, w)\n",
    "        \n",
    "def word_show_nns(word, Wv, vocabulary, k=10):\n",
    "    show_nns(Wv[vocabulary.to_id(word)], Wv, vocabulary, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_embed_matrix_val, word_context_embed_matrix_val = sess.run([word_embed_matrix, word_context_embed_matrix])\n",
    "\n",
    "# As per the paper, we take the average of the word's vector when it's the center word of the window\n",
    "# and the vector when it's found in the context.\n",
    "#\n",
    "# There is some (handwave-y) motivation for why we do this in section 4.2 of GloVe.\n",
    "Wv = word_embed_matrix_val + word_context_embed_matrix_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest neighbors:\n",
      "1.000 : \"one\"\n",
      "0.216 : \"frequencies\"\n",
      "0.205 : \"perceive\"\n",
      "0.203 : \"dictate\"\n",
      "0.196 : \"governmental\"\n",
      "0.194 : \"lacks\"\n",
      "0.192 : \"northfield\"\n",
      "0.189 : \"enthusiastically\"\n",
      "0.187 : \"around\"\n",
      "0.186 : \"keith's\"\n"
     ]
    }
   ],
   "source": [
    "word_show_nns('one', Wv, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_show_nns('king', Wv, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_show_nns('car', Wv, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_show_nns('computer', Wv, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_show_nns('college', Wv, vocabulary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
